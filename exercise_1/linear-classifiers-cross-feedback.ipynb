{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyhDiUqFLzoB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Laura Lecinena Pastor\n",
        "Michael Strick\n",
        "Lisa Fenzel\n",
        "\"\"\"\n",
        "\n",
        "import numpy # matrices and multi-dimensional arrays, linear algebra\n",
        "import sklearn # machine learning\n",
        "import matplotlib # plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGwibk_gL34K"
      },
      "source": [
        "# 1 Exploring the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROwldJ-HL48R",
        "outputId": "28847096-4000-41de-ef77-67b405538bb2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "print(digits.keys())\n",
        "\n",
        "data = digits[\"data\"]\n",
        "images = digits[\"images\"]\n",
        "target = digits[\"target\"]\n",
        "target_names = digits[\"target_names\"]\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPCQrktnL-io"
      },
      "source": [
        "What is the size of these images? - It is a 8x8 array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro7ZNMJaL_cg",
        "outputId": "684a5382-eac9-43cf-dd95-26bf2a1f8c1e"
      },
      "outputs": [],
      "source": [
        "print(np.shape(images[3]))\n",
        "print(np.shape(images[9]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IracCfFyMEjY"
      },
      "source": [
        "Visulize one image of a 3 using the imshow function trying the two interpolation methods in the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "sZKVee4YMHGo",
        "outputId": "cd2a434f-ae50-4415-d44c-db3d794ddffe"
      },
      "outputs": [],
      "source": [
        "img = images[3]\n",
        "\n",
        "assert 2 == len(img.shape)\n",
        "\n",
        "plt.figure()\n",
        "plt.gray()\n",
        "plt.imshow(img, interpolation = \"nearest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "S7GtmXIeMJrw",
        "outputId": "41319b05-913f-4232-a170-089bcf24c32e"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.gray()\n",
        "plt.imshow(img, interpolation = \"bicubic\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "The solutions are the same with the given solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDfPD7XyMLBx"
      },
      "source": [
        "Filter dataset and split it in training and test set (#train/#test=3/2 i.e. test set is 40% of filtered data set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvM20ltWMOVU"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "X_all = data\n",
        "y_all = target\n",
        "\n",
        "filtered_data = []\n",
        "filtered_labels = []\n",
        "\n",
        "for image, label in zip(digits.images, digits.target):\n",
        "    if label == 3 or label == 9:\n",
        "       filtered_data.append(image)\n",
        "       filtered_labels.append(label)\n",
        "\n",
        "filtered_data = np.array(filtered_data)\n",
        "filtered_labels = np.array(filtered_labels)\n",
        "X_train , X_test , y_train , y_test = model_selection.train_test_split(filtered_data, filtered_labels,\n",
        "test_size = 0.4, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "The solutions are the same. However, I suggest rather than using label == 3 or label == 9, you could compare if label in target_list, in which target_list = [3,9]. This would be more flexible for the code if adding more targets is neccessary in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5nYI5mgMUGD"
      },
      "source": [
        "# 2 Hand-crafted classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "he06x9XJMW00",
        "outputId": "896d40c2-e845-4241-c57b-13e6546ac474"
      },
      "outputs": [],
      "source": [
        "average_image_3 = np.mean([image for image, label in zip(filtered_data, filtered_labels) if label == 3], axis = 0)\n",
        "average_image_9 = np.mean([image for image, label in zip(filtered_data, filtered_labels) if label == 9], axis = 0)\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 1, ncols = 2)\n",
        "ax[0].imshow(average_image_3)\n",
        "ax[0].set_title(\"Label 3\")\n",
        "ax[1].imshow(average_image_9)\n",
        "ax[1].set_title(\"Label 9\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjmP4ooQMaAJ",
        "outputId": "2a679d21-8c8f-4de4-8846-7e0c992f362a"
      },
      "outputs": [],
      "source": [
        "def features2d(x):\n",
        "\n",
        "  f_1 = x[0][3] + x[0][4] + 1.5 * x[4][4]\n",
        "  f_2 = x[2][2] + 2 * x[3][2] + x[3][3] + 2 * x[3][5] + 0.5 * x[2][5]\n",
        "\n",
        "  return f_1, f_2\n",
        "\n",
        "print(f\"Mean for 3 in average image: {features2d(average_image_3)}\\nMean for 9 in average image: {features2d(average_image_9)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "Your solution also classifies the number 3 and 9 based on certain pixels and assign weights to them which is nice. So could you explain why you chose those pixels and weights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "mGB6MfImMdyD",
        "outputId": "4adde4bc-abdd-4f7d-a47c-13d66add5f5c"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "\n",
        "f_3x, f_3y, f_9x, f_9y = [], [], [], []\n",
        "training_features = []\n",
        "\n",
        "for item, label in zip(X_train, y_train):\n",
        "  feature = features2d(item)\n",
        "  training_features.append(feature)\n",
        "\n",
        "  if label == 3:\n",
        "    f_3x.append(feature[0])\n",
        "    f_3y.append(feature[1])\n",
        "\n",
        "  if label == 9 :\n",
        "    f_9x.append(feature[0])\n",
        "    f_9y.append(feature[1])\n",
        "\n",
        "mean_3 = (mean(f_3x), mean(f_3y))\n",
        "mean_9 = (mean(f_9x), mean(f_9y))\n",
        "\n",
        "print(f\"Mean for 3 in training set: {mean_3}\\nMean for 9 in training set: {mean_9}\")\n",
        "\n",
        "# Berechnung des Vektors zwischen den Means (von mean 3 zu mean 9)\n",
        "vector = (mean_9[0] - mean_3[0], mean_9[1] - mean_3[1])\n",
        "\n",
        "# Drehung des Vektors um 90 Grad\n",
        "perpendicular = (-vector[1], vector[0])\n",
        "\n",
        "# Berechnung des Punktes auf der Mitte des Vektors\n",
        "\n",
        "bisector_point = (mean_3[0] + 0.5 * vector[0], mean_3[1] + 0.5 * vector[1])\n",
        "\n",
        "x_range = np.linspace(0, 100, 100)\n",
        "y_range = bisector_point[1] + (x_range - bisector_point[0]) * perpendicular[1] / perpendicular[0]\n",
        "plt.plot(x_range, y_range, color = 'green')\n",
        "plt.scatter(f_3x, f_3y, color = 'red')\n",
        "plt.scatter(f_9x, f_9y, color = 'blue')\n",
        "plt.plot(mean_3[0], mean_3[1], color = 'black', marker = 'x', markersize = 10)\n",
        "plt.plot(bisector_point[0], bisector_point[1], color = 'green', marker = 'x')\n",
        "plt.plot(mean_9[0], mean_9[1], color = 'black', marker = 'x', markersize = 10)\n",
        "plt.axis(\"scaled\")\n",
        "plt.xlim(xmin = 0, xmax = 100)\n",
        "plt.ylim(ymin = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "The solutions are the same with the given solutions. However, I think the testing set should be plotted as well. Moreover not sure if we were allowed to import additional libraries, you could've used np.mean()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXM3382TMhoY",
        "outputId": "13d77b04-41f9-4d95-a7d8-009b7265954a"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "predicted_features = []\n",
        "\n",
        "# Definition der Schwellenwertfunktion\n",
        "def decision_boundary(x):\n",
        "  if math.sqrt((mean_3[0] - x[0])**2 + (mean_3[1] - x[1])**2) > math.sqrt((mean_9[0] - x[0])**2 + (mean_9[1] - x[1])**2):\n",
        "    return 9\n",
        "  else:\n",
        "    return 3\n",
        "\n",
        "def nearest_mean(training_features, training_labels, test_features):\n",
        "\n",
        "  N, FP, FN = len(training_features), 0, 0\n",
        "  predicted_labels = []\n",
        "\n",
        "  # Fehler bei Trainingset\n",
        "  for item, label in zip(training_features, training_labels):\n",
        "    prediction = decision_boundary(features2d(item))\n",
        "\n",
        "    if prediction == 3 and label == 9:    # \\hat{y}=3 und y^*=9 d.f. false negative\n",
        "      FN += 1\n",
        "    if prediction == 9 and label == 3:    # \\hat{y}=9 und y^*=3 d.f. false positive\n",
        "      FP += 1\n",
        "\n",
        "  error_ratio = (FP + FN) / N\n",
        "\n",
        "  print(f\"Number of cases: {N}\\nfalse negatives: {FN}\\nfalse positives: {FP}\\nerror ratio: {round(error_ratio, 5)}\")\n",
        "\n",
        "  # Prediction labels of testset\n",
        "  for item in test_features:\n",
        "    feature = features2d(item)\n",
        "    predicted_features.append(feature)\n",
        "    prediction = decision_boundary(feature)\n",
        "    if prediction == 3:\n",
        "      predicted_labels.append(3)\n",
        "    if prediction == 9:\n",
        "      predicted_labels.append(9)\n",
        "\n",
        "  return predicted_labels\n",
        "\n",
        "predicted_labels = nearest_mean(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "The error rate is better than the given solution which is good. But I think the code should be implemented as the given code so that it would be suitable for more than 2 target classes. Instead of importing math, you coud've used np.sqrt()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "_uQ_YildMlAg",
        "outputId": "f1cf62e6-8f8a-4da4-afa5-d460879ff3ff"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "min1, max1, min2, max2 = sys.maxsize, 0, sys.maxsize, 0\n",
        "\n",
        "f_3x, f_3y, f_9x, f_9y = [], [], [], []\n",
        "\n",
        "\n",
        "for item, label in zip(X_test, y_test):\n",
        "  feature = features2d(item)\n",
        "\n",
        "\n",
        "  if label == 3:\n",
        "    f_3x.append(feature[0])\n",
        "    f_3y.append(feature[1])\n",
        "\n",
        "  if label == 9 :\n",
        "    f_9x.append(feature[0])\n",
        "    f_9y.append(feature[1])\n",
        "\n",
        "mean_3 = (mean(f_3x), mean(f_3y))\n",
        "mean_9 = (mean(f_9x), mean(f_9y))\n",
        "\n",
        "for feature in predicted_features:\n",
        "    x, y = feature\n",
        "\n",
        "    if x < min1:\n",
        "        min1 = x\n",
        "    if x > max1:\n",
        "        max1 = x\n",
        "\n",
        "    if y < min2:\n",
        "        min2 = y\n",
        "    if y > max2:\n",
        "        max2 = y\n",
        "\n",
        "x1grid = np.arange(min1 - 1, max1 + 1, 0.1)\n",
        "x2grid = np.arange(min2 - 1, max2 + 1, 0.1)\n",
        "xx, yy = np.meshgrid(x1grid, x2grid)\n",
        "r1, r2 = xx.flatten(), yy.flatten()\n",
        "r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
        "grid = np.hstack((r1,r2))\n",
        "model = LogisticRegression()\n",
        "model.fit(predicted_features, predicted_labels)\n",
        "yhat = model.predict(grid)\n",
        "yhat = model.predict(grid)\n",
        "zz = yhat.reshape(xx.shape)\n",
        "colors = ['#ff7979', '#98d8f8']\n",
        "cmap = ListedColormap(colors)\n",
        "plt.contourf(xx, yy, zz, cmap=cmap)\n",
        "\n",
        "plt.scatter(f_3x, f_3y, color = 'red')\n",
        "plt.scatter(f_9x, f_9y, color = 'blue')\n",
        "plt.plot(mean_3[0], mean_3[1], color = 'black', marker = 'x', markersize = 10)\n",
        "plt.plot(bisector_point[0], bisector_point[1], color = 'green', marker = 'x')\n",
        "plt.plot(mean_9[0], mean_9[1], color = 'black', marker = 'x', markersize = 10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "I believe we do not need 4 arrays to store coordinates of features for 3 and 9 because we can create a 2-d array to store values of features2d(X_test) and work on that. Furthermore you could've used np.where to divide 3s and 9s since it is faster than looping in python. Instead of importing sys and using the system max integer, it would have been enough to use the maximum number the feature function can return, or better yet, avoid any python loops as they are in general slower than the in c optimized numpy functions and just use np.min() and np.max()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tc2vGrQMoIn"
      },
      "source": [
        "# 3 LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuWXxQ_-MyBB",
        "outputId": "8a097963-42fc-47a0-8824-bab79e11c4ab"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def fit_lda(training_features, training_labels):\n",
        "\n",
        "    mu_3, mu_9, mu_3_count, mu_9_count = 0, 0, 0, 0\n",
        "\n",
        "    for item, label in zip(training_features, training_labels):\n",
        "        if label == 3:\n",
        "            mu_3 += item\n",
        "            mu_3_count +=1\n",
        "\n",
        "        if label == 9 :\n",
        "            mu_9 += item\n",
        "            mu_9_count +=1\n",
        "\n",
        "    mu = [1 / mu_3_count* mu_3, 1 / mu_9_count* mu_9]\n",
        "    mu_3_1 = [1 / mu_3_count* mu_3]\n",
        "\n",
        "\n",
        "    N=len(training_features)\n",
        "    D=len(np.array(training_features).T)\n",
        "\n",
        "    # covariance matrix\n",
        "    covmat=np.zeros((D,D))\n",
        "\n",
        "    for item, label in zip(training_features, training_labels):\n",
        "        if label == 3:\n",
        "            s=np.array(item-mu[0])\n",
        "            covmat+= np.outer(s,s)\n",
        "        if label == 9:\n",
        "            t=np.array(item-mu[1])\n",
        "            covmat+= np.outer(t,t)\n",
        "\n",
        "    covmat= (1/N)*covmat\n",
        "\n",
        "    # propabilities\n",
        "    p=[mu_3_count/N, mu_9_count/N ]\n",
        "\n",
        "    #cov_matrix = np.cov(mu, rowvar=False)\n",
        "    #print(cov_matrix)\n",
        "    #return mu, covmat, p\n",
        "    return mu, covmat, p\n",
        "\n",
        "\n",
        "mu, covmat,p = fit_lda(np.array(training_features),y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_lda(mu, covmat, p, test_features):\n",
        "    beta=np.dot(np.linalg.inv(covmat),np.array(mu[1]-mu[0]))\n",
        "    b=-0.5*np.dot(mu[1]+mu[0],beta)+ math.log(p[1]/p[0])\n",
        "\n",
        "    # total number of test instances\n",
        "    M=len(test_features)\n",
        "    # total number of test features\n",
        "    D=len(np.array(test_features).T)\n",
        "\n",
        "    prediction =[]\n",
        "\n",
        "    for item in test_features:\n",
        "        if np.dot(np.array(item).T,beta) >(-1)*b:\n",
        "            prediction.append(9)\n",
        "        else:\n",
        "            prediction.append(3)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels_training = predict_lda(mu, covmat,p,training_features)\n",
        "N, FP, FN, k = len(np.array(X_train)), 0, 0, 0\n",
        "\n",
        "for item, label in zip(training_features, y_train):\n",
        "    prediction = predicted_labels_training[k]\n",
        "    if prediction == 3 and label == 9:    # \\hat{y}=3 und y^*=9 d.f. false negative\n",
        "      FN += 1\n",
        "    if prediction == 9 and label == 3:    # \\hat{y}=9 und y^*=3 d.f. false positive\n",
        "      FP += 1\n",
        "    k +=1\n",
        "\n",
        "error_ratio_train = (FP + FN) / N\n",
        "print(error_ratio_train)\n",
        "\n",
        "\n",
        "#error rate test data\n",
        "N, FP, FN, k = len(predicted_features), 0, 0, 0\n",
        "predicted_labels_test = predict_lda(mu, covmat, p, np.array(predicted_features))\n",
        "for item, label in zip(np.array(predicted_features), y_test):\n",
        "    prediction = predicted_labels_test[k]\n",
        "    if prediction == 3 and label == 9:    # \\hat{y}=3 und y^*=9 d.f. false negative\n",
        "      FN += 1\n",
        "    if prediction == 9 and label == 3:    # \\hat{y}=9 und y^*=3 d.f. false positive\n",
        "      FP += 1\n",
        "    k +=1\n",
        "\n",
        "error_ratio_test = (FP + FN) / N\n",
        "print(error_ratio_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "- Implement LDA Training: For the calculation of the means of the 3s and 9s, I would recommend the np.mean() function since it is faster and more readable.\n",
        "- Implement LDA Prediction: It's basically like the sample solution, even though you could use np.sign to get the predicted labels (And maybe delete the unused variables D and M). To make it more efficient, you could use the dot product on all test_features and beta at once, add b, use the sign function and use the result -1 or 1 as labels in your prediction list instead of looping through the list.\n",
        "- Error rate calculation: Good approach, but you could use the np.mean() function and compare them to y_test/y_train to for a less complicated solution since we don't differentiate between FP and FN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afpvD8FWM1-z"
      },
      "source": [
        "cluster means:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mu_{3}&= \\frac{1}{N_{3}}\\sum_{i:~~y_i^*=3}x_i\n",
        "\\end{aligned}\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mu_{9}&= \\frac{1}{N_{9}}\\sum_{i:~~y_i^*=9}x_i\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "EjHv5M54M1Mj",
        "outputId": "12ca68b3-d38a-4d8a-f3d1-bdac5ca196a4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "print(predicted_labels_training)\n",
        "\n",
        "\n",
        "min1, max1, min2, max2 = sys.maxsize, 0, sys.maxsize, 0\n",
        "\n",
        "f_3x, f_3y, f_9x, f_9y = [], [], [], []\n",
        "training_features = []\n",
        "\n",
        "for item, label in zip(X_train, y_train):\n",
        "  feature = features2d(item)\n",
        "  training_features.append(feature)\n",
        "\n",
        "  if label == 3:\n",
        "    f_3x.append(feature[0])\n",
        "    f_3y.append(feature[1])\n",
        "\n",
        "  if label == 9 :\n",
        "    f_9x.append(feature[0])\n",
        "    f_9y.append(feature[1])\n",
        "\n",
        "mean_3 = (mean(f_3x), mean(f_3y))\n",
        "mean_9 = (mean(f_9x), mean(f_9y))\n",
        "\n",
        "for feature in predicted_features:\n",
        "    x, y = feature\n",
        "\n",
        "    if x < min1:\n",
        "        min1 = x\n",
        "    if x > max1:\n",
        "        max1 = x\n",
        "\n",
        "    if y < min2:\n",
        "        min2 = y\n",
        "    if y > max2:\n",
        "        max2 = y\n",
        "\n",
        "x1grid = np.arange(min1 - 1, max1 + 1, 0.1)\n",
        "x2grid = np.arange(min2 - 1, max2 + 1, 0.1)\n",
        "xx, yy = np.meshgrid(x1grid, x2grid)\n",
        "r1, r2 = xx.flatten(), yy.flatten()\n",
        "r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
        "grid = np.hstack((r1,r2))\n",
        "model = LogisticRegression()\n",
        "model.fit(training_features, predicted_labels_training)\n",
        "yhat = model.predict(grid)\n",
        "yhat = model.predict(grid)\n",
        "zz = yhat.reshape(xx.shape)\n",
        "colors = ['#ff7979', '#98d8f8']\n",
        "cmap = ListedColormap(colors)\n",
        "plt.contourf(xx, yy, zz, cmap=cmap)\n",
        "\n",
        "plt.scatter(f_3x, f_3y, color = 'red')\n",
        "plt.scatter(f_9x, f_9y, color = 'blue')\n",
        "plt.plot(mean_3[0], mean_3[1], color = 'black', marker = 'x', markersize = 10)\n",
        "plt.plot(bisector_point[0], bisector_point[1], color = 'green', marker = 'x')\n",
        "plt.plot(mean_9[0], mean_9[1], color = 'black', marker = 'x', markersize = 10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "- For the plotting, same as above.\n",
        "- I think 3.4 is missing completely as well as the visualization of the cluster shape using ellipses and the drawing of the principal axis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ1RRDuLM_GP"
      },
      "source": [
        "# 4 SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2hoVThIzM1QZ",
        "outputId": "b07a3498-61fb-4e57-cb72-e96fec65c676"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def beta_derivative(train_feat,train_label,N,lam,beta,b):\n",
        "    # training data tuples\n",
        "    train_data = zip(train_feat,train_label)\n",
        "    label_feature_sum = 0\n",
        "    for feature,label in train_data:\n",
        "        feat_ary = np.array(feature)\n",
        "        if label*(feat_ary.dot(beta)+b) < 1:\n",
        "            label_feature_sum += (-label*feat_ary.T)\n",
        "    return beta+lam/N*label_feature_sum\n",
        "\n",
        "def b_derivative(train_feat,train_label,N,lam,beta,b):\n",
        "    # training data tuples\n",
        "    train_data = zip(train_feat,train_label)\n",
        "    label_sum = 0\n",
        "    for feature,label in train_data:\n",
        "        feat_ary = np.array(feature)\n",
        "        if label*(feat_ary.dot(beta)+b) < 1:\n",
        "            label_sum += (-label)\n",
        "    return lam/N*label_sum\n",
        "\n",
        "def gradient_descent(train_feat,train_label,N,lam,beta,b,tau):\n",
        "    # Get new beta and b using gradient-descent\n",
        "    new_beta = beta - tau*beta_derivative(train_feat,train_label,N,lam,beta,b)\n",
        "    new_b = b - tau*b_derivative(train_feat,train_label,N,lam,beta,b)\n",
        "    return new_beta, new_b\n",
        "\n",
        "def fit_svm(train_feat, train_label, lam):\n",
        "    # training data tuples\n",
        "    train_data = zip(train_feat,train_label)\n",
        "\n",
        "    # initialize beta with gaussian normal\n",
        "    beta = np.random.normal(0,1,len(train_feat[0]))\n",
        "    b = 0\n",
        "    relu_sum = 0\n",
        "    #lam = 1 #1\n",
        "    loss = []\n",
        "    steps = 100 # 1000\n",
        "    tau = 0.001  #0.001\n",
        "    i = 0\n",
        "    N = len(train_feat)\n",
        "    wrong_predictions = 0\n",
        "    train_error = []\n",
        "    for x in range(steps):\n",
        "        train_data = zip(train_feat,train_label)\n",
        "        #if x == 50 or x == 75:\n",
        "        #    tau /= 10\n",
        "        for feature,label in train_data:\n",
        "            feat_ary = np.array(feature)\n",
        "            if 1-(label*(feat_ary.dot(beta)+b)) >= 0:\n",
        "            #if (label*(feat_ary.dot(beta)+b)) <= 1:\n",
        "                #print(\"wrong:\",label,feat_ary.dot(beta)+b)\n",
        "                relu_sum+=1-(label*(feat_ary.dot(beta)+b))\n",
        "                wrong_predictions += 1\n",
        "        loss.append(0.5*beta.T.dot(beta)+lam/N*relu_sum)\n",
        "        train_error.append(wrong_predictions/N)\n",
        "        if len(train_error)>50:\n",
        "            #print(abs(train_error[-1]-np.mean(train_error[11:-1])))\n",
        "            if abs(train_error[-1]-np.mean(train_error[11:-1]))<0.1:\n",
        "                tau /= 10\n",
        "        beta, b = gradient_descent(train_feat,train_label,N,lam,beta,b,tau)\n",
        "        wrong_predictions = 0\n",
        "        relu_sum = 0\n",
        "    return beta, b, loss, train_error\n",
        "\n",
        "\n",
        "def predict_svm(beta, b, test_features):\n",
        "    prediction =[]\n",
        "    for item in test_features:\n",
        "        if np.sign(item.dot(beta)+b) > 0:\n",
        "            prediction.append(9)\n",
        "        else:\n",
        "            prediction.append(3)\n",
        "    return prediction\n",
        "\n",
        "def sklearn_svm(training_features,training_labels):\n",
        "    feat_ary = np.array(training_features)\n",
        "    classifier = svm.SVC(kernel='linear').fit(feat_ary,training_labels)\n",
        "    return classifier\n",
        "\n",
        "\n",
        "training_labels = [-1 if item == 3 else 1 for item in y_train]\n",
        "training_features =[]\n",
        "\n",
        "\n",
        "for item, label in zip(X_train, y_train):\n",
        "  feature = features2d(item)\n",
        "  training_features.append(feature)\n",
        "\n",
        "# Optimize beta and b\n",
        "beta, b, loss, train_error = fit_svm(training_features,training_labels,1)\n",
        "\n",
        "\n",
        "test_labels = [-1 if item == 3 else 1 for item in y_train]\n",
        "test_features =[]\n",
        "\n",
        "# Test data prediction\n",
        "for item, label in zip(X_test, y_test):\n",
        "  feature = features2d(item)\n",
        "  test_features.append(feature)\n",
        "\n",
        "test_prediction = predict_svm(beta, b, np.array(test_features))\n",
        "\n",
        "#error rate test data\n",
        "N, FP, FN, k = len(test_features), 0, 0, 0\n",
        "for prediction, label in zip(test_prediction, y_test):\n",
        "    #print(prediction)\n",
        "    if prediction == 3 and label == 9:    # \\hat{y}=3 und y^*=9 d.f. false negative\n",
        "      FN += 1\n",
        "    if prediction == 9 and label == 3:    # \\hat{y}=9 und y^*=3 d.f. false positive\n",
        "      FP += 1\n",
        "error_ratio_test = (FP + FN) / N\n",
        "print(\"Error rate our svm:\", error_ratio_test)\n",
        "\n",
        "\n",
        "classifier = sklearn_svm(training_features,training_labels)\n",
        "y_pred = classifier.predict(np.array(test_features))\n",
        "test_labels = [-1 if item == 3 else 1 for item in y_test]\n",
        "accuracy = accuracy_score(test_labels,y_pred)\n",
        "svm_error = 1-accuracy\n",
        "print(\"Error rate sklearn svm:\",svm_error)\n",
        "\n",
        "\n",
        "# Loss and train error\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('Training error and loss')\n",
        "axs[0].plot(train_error)\n",
        "axs[0].set_title('Train error')\n",
        "axs[1].plot(loss)\n",
        "axs[1].set_title('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Decision boundary\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle('Decision boundary')\n",
        "sns.scatterplot(x=np.array(training_features)[:, 0],\n",
        "                y=np.array(training_features)[:, 1],\n",
        "                hue=y_train,\n",
        "                s=8)\n",
        "\n",
        "\n",
        "x_points = np.linspace(0, 100)\n",
        "y_points = -(beta[0] / beta[1]) * x_points - b / beta[1]\n",
        "plt.plot(x_points, y_points, c='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "The code of the SVM is essentially the same as in the solution. Many optimizations can be made by basically removing every loop through the data points and replacing it with more efficient numpy functions as mentioned with some examples above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vLrLGeO8NJii",
        "outputId": "1f1d3d73-4a26-4fa8-da15-d8f0de8ee15b"
      },
      "outputs": [],
      "source": [
        "beta, b, loss, train_error = fit_svm(training_features,training_labels,0)\n",
        "\n",
        "# Decision boundary\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle('Decision boundary Lambda = 0')\n",
        "sns.scatterplot(x=np.array(training_features)[:, 0],\n",
        "                y=np.array(training_features)[:, 1],\n",
        "                hue=y_train,\n",
        "                s=8)\n",
        "\n",
        "\n",
        "x_points = np.linspace(0, 100)\n",
        "y_points = -(beta[0] / beta[1]) * x_points - b / beta[1]\n",
        "plt.plot(x_points, y_points, c='r')\n",
        "\n",
        "\n",
        "beta, b, loss, train_error = fit_svm(training_features,training_labels,0.1)\n",
        "\n",
        "# Decision boundary\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle('Decision boundary Lambda = 0.1')\n",
        "sns.scatterplot(x=np.array(training_features)[:, 0],\n",
        "                y=np.array(training_features)[:, 1],\n",
        "                hue=y_train,\n",
        "                s=8)\n",
        "\n",
        "\n",
        "x_points = np.linspace(0, 100)\n",
        "y_points = -(beta[0] / beta[1]) * x_points - b / beta[1]\n",
        "plt.plot(x_points, y_points, c='r')\n",
        "\n",
        "beta, b, loss, train_error = fit_svm(training_features,training_labels,1)\n",
        "\n",
        "# Decision boundary\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle('Decision boundary Lambda = 1')\n",
        "sns.scatterplot(x=np.array(training_features)[:, 0],\n",
        "                y=np.array(training_features)[:, 1],\n",
        "                hue=y_train,\n",
        "                s=8)\n",
        "\n",
        "\n",
        "x_points = np.linspace(0, 100)\n",
        "y_points = -(beta[0] / beta[1]) * x_points - b / beta[1]\n",
        "plt.plot(x_points, y_points, c='r')\n",
        "\n",
        "\n",
        "beta, b, loss, train_error = fit_svm(training_features,training_labels,10)\n",
        "\n",
        "# Decision boundary\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle('Decision boundary Lambda = 10')\n",
        "sns.scatterplot(x=np.array(training_features)[:, 0],\n",
        "                y=np.array(training_features)[:, 1],\n",
        "                hue=y_train,\n",
        "                s=8)\n",
        "\n",
        "\n",
        "x_points = np.linspace(0, 100)\n",
        "y_points = -(beta[0] / beta[1]) * x_points - b / beta[1]\n",
        "plt.plot(x_points, y_points, c='r')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
        "\n",
        "The SVM achieves good results. A little bit of code cleanup after the DRY principle would have been nice. Additionally, instead of using an extra library seaborn, you could have used matplotlib which offers in this case the same functionality."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
