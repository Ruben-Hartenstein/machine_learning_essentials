{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy # matrices and multi - dimensional arrays , linear algebra\n",
    "import sklearn # machine learning\n",
    "import matplotlib # plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.keys())\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = images[3]\n",
    "\n",
    "assert 2 == len(img.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.gray()\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(img, interpolation='bicubic') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "The solutions are essentialy the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_indices = np.where(np.isin(target, [3, 9]))\n",
    "\n",
    "# Use the filtered indices to get the corresponding elements from data, images, and target\n",
    "data = data[filter_indices]\n",
    "images = images[filter_indices]\n",
    "target = target[filter_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_all = data\n",
    "y_all = target\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data, target, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "The solutions are essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features2d(x):\n",
    "    features = np.zeros((x.shape[0], 2))\n",
    "    for i in range(x.shape[0]):\n",
    "        features[i, 0] = x[i, 10] + x[i, 18] + x[i, 26]\n",
    "        features[i, 1] = x[i, 26] + x[i, 27] + x[i, 28]\n",
    "       \n",
    "    return features\n",
    "\n",
    "features_all = features2d(X_all)\n",
    "features_train = features2d(X_train)\n",
    "features_test = features2d(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "The solutions are essentially the same. Both solutions look at the pixel values of two areas in the image, but our solution doesn't take the mean, which shouldn't affect it's potency. Our features are able to distinguish between 3s and 9s, but could have definitely been better chosen. We could have worked on a more sophisticated way to try to differentiate between the numbers, or we could have also examined our training data to see which pixels vary on average the most between 3s and 9s to give the pixels weights for their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_with_colors(points, labels):\n",
    "    # Extract x and y coordinates\n",
    "    x_coords = points[:, 0]\n",
    "    y_coords = points[:, 1]\n",
    "    \n",
    "    # Create a mask for points labeled as 3 and 9\n",
    "    mask_3 = (labels == 3)\n",
    "    mask_9 = (labels == 9)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x_coords[mask_3], y_coords[mask_3], marker='o', color='blue', label='3', alpha=0.5)\n",
    "    plt.scatter(x_coords[mask_9], y_coords[mask_9], marker='o', color='red', label='9', alpha=0.5)\n",
    "    plt.title('Scatter Plot of Training Features')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_points_with_colors(features_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "The solutions are essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_mean(X_train, y_train, X_test):\n",
    "\n",
    "    #Compute the mean of each class\n",
    "    mean_3 = np.mean(X_train[y_train == 3], axis=0)\n",
    "    mean_9 = np.mean(X_train[y_train == 9], axis=0)\n",
    "    \n",
    "    # Compute the Euclidean distance from each test point to the mean of each class\n",
    "    distance_3 = np.linalg.norm(X_test - mean_3, axis=1)\n",
    "    distance_9 = np.linalg.norm(X_test - mean_9, axis=1)\n",
    "    \n",
    "    # Classify test points based on the minimum distance\n",
    "    test_labels = np.where(distance_3 < distance_9, -1, 1)\n",
    "    \n",
    "    return test_labels, mean_3, mean_9\n",
    "\n",
    "def calculate_error_rate(predicted_labels, true_labels):\n",
    "    total_sample = len(true_labels)\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    for i in range(total_sample):\n",
    "        if predicted_labels[i] != -1 and true_labels[i] == 3:\n",
    "            fn += 1\n",
    "        elif predicted_labels[i] != 1 and true_labels[i] == 9:\n",
    "            fp += 1\n",
    "    \n",
    "    return (fp + fn) / total_sample  \n",
    "\n",
    "predicted_labels_mean, mean_3, mean_9 = nearest_mean(features_train, y_train, features_test)\n",
    "error_rate_mean = calculate_error_rate(predicted_labels_mean, y_test)\n",
    "print(f\"Error Rate Nearest Mean: {error_rate_mean*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "Both solutions of the nearest_mean() function fulfill the task but ours, while I find it to be more readable, is only meant for two features whereas the provided solution can be used for any amount of features. Also by separating the distance_from_mean() function it could also be passed as a callback function to use different distance metrics and make it more modular.\n",
    "\n",
    "The error rate calculation is more sophisticated in the given solution. I have decided to implement it differently to be able to see if more 3s are identified as 9s or the other way around but since I never used this due to time constraints there is no point to our slower implementation.\n",
    "\n",
    "I wouldn't say 11,46% is a bad result but there is a lot room for improvement simply by tweaking on our features.\n",
    "\n",
    "At this point the task was a little bit confusing and I was not sure if I am supposed to implement a linear classifier. Originally I did implement one with a simple grid-based hyperoptimization but decided to delete it since it wasn't asked for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 200x200 grid\n",
    "x_min, x_max = 0, 50\n",
    "y_min, y_max = 0, 50\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "grid_values = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "distance_3 = np.linalg.norm(grid_values - mean_3, axis=1)\n",
    "distance_9 = np.linalg.norm(grid_values - mean_9, axis=1)\n",
    "    \n",
    "# Classify test points based on the minimum distance\n",
    "decision_region = np.where(distance_3 < distance_9, -1, 1)\n",
    "\n",
    "# Extract x and y coordinates from grid_values\n",
    "x_coords = grid_values[:, 0]\n",
    "y_coords = grid_values[:, 1]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_coords, y_coords, c=np.where(decision_region == -1, 'blue', 'red'), marker='o', alpha=0.5)\n",
    "plt.scatter(features_test[y_test == 3, 0], features_test[y_test == 3, 1], c='green', marker='o', label='3')\n",
    "plt.scatter(features_test[y_test == 9, 0], features_test[y_test == 9, 1], c='yellow', marker='o', label='9')\n",
    "plt.scatter(mean_3[0], mean_3[1], c='black', marker='o', label='Mean 3', linewidth=4)\n",
    "plt.scatter(mean_9[0], mean_9[1], c='black', marker='*', label='Mean 9', linewidth=4)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Region of Nearest Mean Classifier')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "The solutions are essentially the same. But I have to say that the plotting style of the given solution looks a lot better. Since I've deleted our linear classifier we have only one plot here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lda(X_train, y_train):\n",
    "    # Compute the mean of each class\n",
    "    mu_3 = np.mean(X_train[y_train == 3], axis=0)\n",
    "    mu_9 = np.mean(X_train[y_train == 9], axis=0)\n",
    "    mu = np.array([mu_3, mu_9])\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    cov = 0\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        if y == 3:\n",
    "            x_i = x - mu_3\n",
    "        else:    \n",
    "            x_i = x - mu_9\n",
    "        cov += np.outer(x_i, np.transpose(x_i))\n",
    "\n",
    "    covmat = cov/len(y_train)\n",
    "    # Compute the prior probability of each class\n",
    "    p = np.array([np.sum(y_train == 3) / len(y_train), np.sum(y_train == 9) / len(y_train)])\n",
    "\n",
    "    return mu, covmat, p\n",
    "\n",
    "def filter_pixels(X_train, X_test, var=0.001):\n",
    "    pixel_variances = np.var(X_train, axis=0)\n",
    "    # Find pixels with variance smaller than 0.001\n",
    "    dead_pixels_mask = pixel_variances < var\n",
    "    # Filter out dead pixels\n",
    "    X_train_filtered = X_train[:, ~dead_pixels_mask]\n",
    "    X_test_filtered = X_test[:, ~dead_pixels_mask]\n",
    "    return X_train_filtered, X_test_filtered\n",
    "\n",
    "X_train_filtered, X_test_filtered = filter_pixels(X_train, X_test)\n",
    "feature_mu, feature_covmat, feature_p = fit_lda(features_train, y_train)\n",
    "var_mu, var_covmat, var_p = fit_lda(X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "Both implementations are correct, but again the provided solution is designed for any number of features while ours only can handle two specific features. Also the computation of the covariance matrix by subtracting the class means from the entire training feature set instead of doing it iteratively should be way faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lda(mu, covmat, p, test_features):\n",
    "    # Compute the inverse of the covariance matrix\n",
    "    covmat_inv = np.linalg.inv(covmat)\n",
    "\n",
    "    beta = np.dot(covmat_inv, np.transpose(mu[1] - mu[0]))\n",
    "    b = -0.5 * np.dot((mu[0] + mu[1]), beta) + np.log(p[1] / p[0])\n",
    "    # Compute decision function scores\n",
    "    decision_scores = np.dot(test_features, beta) + b\n",
    "    # Classify test points based on the decision function scores\n",
    "    predicted_labels = np.sign(decision_scores)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "predicted_labels_feature_test = predict_lda(feature_mu, feature_covmat, feature_p, features_test)   \n",
    "predicted_labels_var_test = predict_lda(var_mu, var_covmat, var_p, X_test_filtered)\n",
    "predicted_labels_feature_train = predict_lda(feature_mu, feature_covmat, feature_p, features_train)   \n",
    "predicted_labels_var_train = predict_lda(var_mu, var_covmat, var_p, X_train_filtered)\n",
    "\n",
    "error_rate_lda_feature_test = calculate_error_rate(predicted_labels_feature_test, y_test)\n",
    "error_rate_lda_var_test = calculate_error_rate(predicted_labels_var_test, y_test)\n",
    "error_rate_lda_feature_train = calculate_error_rate(predicted_labels_feature_train, y_train)\n",
    "error_rate_lda_var_train = calculate_error_rate(predicted_labels_var_train, y_train)\n",
    "\n",
    "print(f\"Error Rate LDA Feature Test: {error_rate_lda_feature_test*100:.2f}%\")\n",
    "print(f\"Error Rate LDA Variance Test: {error_rate_lda_var_test*100:.2f}%\")\n",
    "print(f\"Error Rate LDA Feature Train: {error_rate_lda_feature_train*100:.2f}%\")\n",
    "print(f\"Error Rate LDA Variance Train: {error_rate_lda_var_train*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the distinct error rates obtained from our implementation of LDA. The four values represent various combinations of LDA error rates: one with our handcrafted features and another with pixel data exhibiting sufficiently high variance, each tested using our separate test and train datasets. This data suggests that our handcrafted features are not very good in differentiating between 3's and 9's. Another noticeable characteristic is that the Error rate for our features is higher for the Train data then it is for the Test data. This is normally not the case but it could be a coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "Both solutions are essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "features = [features_train, features_test]\n",
    "y = [y_train, y_test]\n",
    "titles = [\"Decision Region of LDA Training Data\", \"Decision Region of LDA Test Data\"]\n",
    "for i in range(len(features)):\n",
    "\n",
    "    # Create a 200x200 grid\n",
    "    x_min, x_max = 0, 50\n",
    "    y_min, y_max = 0, 50\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    grid_values = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Classify test points based on the decision boundary\n",
    "    decision_region = predict_lda(feature_mu, feature_covmat, feature_p, grid_values) \n",
    "\n",
    "    # Extract x and y coordinates from grid_values\n",
    "    x_coords = grid_values[:, 0]\n",
    "    y_coords = grid_values[:, 1]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x_coords, y_coords, c=np.where(decision_region == -1, 'blue', 'red'), marker='o', alpha=0.5)\n",
    "\n",
    "    plt.scatter(features[i][y[i] == 3, 0], features[i][y[i] == 3, 1], c='green', marker='o', label='3')\n",
    "    plt.scatter(features[i][y[i] == 9, 0], features[i][y[i] == 9, 1], c='yellow', marker='o', label='9')\n",
    "\n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(feature_covmat)\n",
    "\n",
    "    # Plot each Gaussian distribution\n",
    "    for j in range(len(feature_mu)):\n",
    "        # Compute PDF values for the grid points\n",
    "        pdf_values = multivariate_normal.pdf(grid_values, mean=feature_mu[j], cov=feature_covmat)\n",
    "\n",
    "        # Reshape PDF values to match grid shape\n",
    "        pdf_values = pdf_values.reshape(xx.shape)\n",
    "\n",
    "        # Plot isocontours (ellipses)\n",
    "        plt.contour(xx, yy, pdf_values, colors='k')\n",
    "\n",
    "        # Plot mean of the Gaussian distribution\n",
    "        plt.plot(feature_mu[j][0], feature_mu[j][1], 'ko')\n",
    "\n",
    "        # Compute standard deviations along each principal component (square roots of eigenvalues)\n",
    "        std_devs = np.sqrt(eigenvalues)\n",
    "\n",
    "        # Get principal cluster axes (eigenvectors)\n",
    "        principal_axes = eigenvectors.T[j] * std_devs[j]  # Scale eigenvectors by standard deviations\n",
    "\n",
    "        # Plot principal cluster axes\n",
    "        plt.plot([feature_mu[j][0], feature_mu[j][0] + principal_axes[0]], [feature_mu[j][1], feature_mu[j][1] + principal_axes[1]], 'k--', linewidth=3)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(titles[i])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the points for the LDA with the Training data look like they are closer to the mean of the gaussian distribution, the actual error rate is higher for the LDA with the Training data which is unusual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "Both implementations visualize the decision regions and the mean of each class. But in our implementation we visualize the cluster shape using ellipses. Unfortunately, I made a mistake when drawing radii of the clusters, I only printed one for each mean value.\n",
    "\n",
    "Instead of the line:\n",
    "\n",
    "plt.plot([feature_mu[j][0], feature_mu[j][0] + principal_axes[0]],\n",
    "         [feature_mu[j][1], feature_mu[j][1] + principal_axes[1]], 'k', linewidth=3)\n",
    "\n",
    "It should have been:\n",
    "\n",
    "plt.plot([feature_mu[0][0] - principal_axes[0], feature_mu[0][0] + principal_axes[0]],\n",
    "         [feature_mu[0][1] - principal_axes[1], feature_mu[0][1] + principal_axes[1]], 'k', linewidth=3)\n",
    "        \n",
    "plt.plot([feature_mu[1][0] - principal_axes[0], feature_mu[1][0] + principal_axes[0]],\n",
    "         [feature_mu[1][1] - principal_axes[1], feature_mu[1][1] + principal_axes[1]], 'k', linewidth=3)\n",
    "\n",
    "Otherwise, due to the iterator j I'm only plotting the first radius for the first mean and only the second one for the second mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "estimator = LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "scores = cross_val_score(estimator, X_all, y_all, cv=10)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn implementation of the LDA reaches even for all 64 pixels used as feature a pretty high accuracy of 0.978. This, as well as the different Error Rates from our implementation would suggest that our features are not very well selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "Opposed to the given solution we use all available data instead of again filtering for dead pixels. Of course it is better to reduce the dimensions especially when it is free like this but based on the given task we thought we should use all the available data. Futhermore, as the task suggests we used the sklearn implementation of the cross_val_score and the LinearDisciminantAnalysis. At the end we take the average of the accuracies and compare it to our own implementation of the LDA without cross validation.\n",
    "\n",
    "If we would want to use the sklearn implementation of the cross_val_score() function with our own LDA fit and predict functions we would have needed to adapt our code, wrap it inside a Class classifier and provide the needed APIs like fit(), predict(), score(), and some more, which we didn't do since we weren't sure if it was part of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta_loss(X, y, beta, b, _lambda):\n",
    "    sum_b = 0\n",
    "    sum_beta = 0\n",
    "    for i in range(len(y)):\n",
    "            if y[i] * (np.dot(X[i], beta) + b) < 1:\n",
    "                sum_beta -= y[i] * np.transpose(X[i])\n",
    "                sum_b -= y[i]\n",
    "    delta_b = (_lambda / len(y)) * sum_b\n",
    "    delta_beta = beta + (_lambda / len(y)) * sum_beta\n",
    "\n",
    "    return delta_beta, delta_b\n",
    "\n",
    "def fit_svm(training_features, training_labels, _lambda, beta, b, tau):\n",
    "    _sum = 0\n",
    "    for i in range(len(training_labels)):\n",
    "        _sum += np.maximum(1 - training_labels[i] * (np.dot(training_features[i], beta)) + b, 0)\n",
    "\n",
    "    loss = 0.5 * np.dot(np.transpose(beta), beta) + (_lambda / len(training_labels)) * _sum\n",
    "\n",
    "    delta_beta, delta_b = calculate_delta_loss(training_features, training_labels, beta, b, _lambda)\n",
    "    beta -= tau * delta_beta\n",
    "    b -= tau * delta_b\n",
    "\n",
    "    return loss, beta, b\n",
    "\n",
    "def predict_svm(test_features, beta, b):\n",
    "    return np.sign(np.dot(test_features, beta) + b)\n",
    "\n",
    "\n",
    "tau = 0.0001\n",
    "_lambda = 10\n",
    "#beta = np.random.normal(0, 1, training_features.shape[1])\n",
    "#b = 0\n",
    "beta = np.array([0.24034121, 0.08780151])\n",
    "b = -8.948143239056721\n",
    "\n",
    "epochs = 1000\n",
    "error_rate_list = []\n",
    "loss_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i == 900:\n",
    "        tau /= 10\n",
    "    predicted_labels = predict_svm(features_test, beta, b)\n",
    "    error_rate = calculate_error_rate(predicted_labels, y_test)\n",
    "    loss, beta, b = fit_svm(features_train, y_train, _lambda, beta, b, tau)\n",
    "    error_rate_list.append(error_rate*100)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "plt.plot(range(epochs), error_rate_list, label='Error Rate')\n",
    "plt.plot(range(epochs), loss_list, label='Loss')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Error Rate and Loss vs Iteration')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "The implementation for predict_svm() is essentially the same as well as the initial values for beta and b (at least at the beginning, later we tried with already good initial values). In the beginning we also had 0.01 or 0.001 for tau but our fit_svm() function immediately butchered the prediction after the first iteration, hence we set the value to 0.0001 to try to inspect why this is. Unfortuately we couldn't pinpoint our mistake. Now with the given solution we can inspect this in more detail and there are a few difference I can spot.\n",
    "\n",
    "1. In the given solution, in the computation of the loss the dot product of beta and beta is calculated whereas we calculate the dot product of beta. Since beta is a 1-dimensional array, the results should be the same and you can skip the arithmetic work of transposing. \n",
    "2. We pass the test data to the predict_svm() function instead of the training data which we probably did out of habit but since it is part of the training we should pass the training data.\n",
    "3. In our implementation of the gradient descent algorithm, we divide by len(y) which isn't correct since we should only divde by the amount of samples that actually fulfill the margin requirement.\n",
    "\n",
    "Altough the given solution uses much faster numpy functionality for the calculation of Loss and Gradient, I can't find any essential difference between the two implementations. Even when changing the differences I've found, our implementation of the SVM does not converge against a good error rate but rather against 50%. This is because there are about as many 3s as there are 9s in that dataset and our resulting LinearClassifier of the SVM just puts every data point on the same side.\n",
    "\n",
    "The graphs of the Loss and Error rate are basically the same but we put both in one graph and our Error rate does not approach a value near 0.\n",
    "\n",
    "Since we've made the mistake to use our test data during the training phase we did not additionally test the end result with the end result. Our Test error SVM value would just be the last value in the error_rate_list. Also since our beta and b values were nonsense, we did not calculate the Training error SVM (it's going to be around the same value as the Test error SVM for the same reasons).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 200x200 grid\n",
    "x_min, x_max = 0, 50\n",
    "y_min, y_max = 0, 50\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "grid_values = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Classify test points based on the decision boundary\n",
    "decision_region = predict_svm(grid_values, beta, b)\n",
    "\n",
    "# Extract x and y coordinates from grid_values\n",
    "x_coords = grid_values[:, 0]\n",
    "y_coords = grid_values[:, 1]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_coords, y_coords, c=np.where(decision_region == -1, 'blue', 'red'), marker='o', alpha=0.5)\n",
    "plt.scatter(features_train[:, 0], features_train[:, 1], c=np.where(y_train == 3, 'green', 'yellow'), marker='o', label='Training Data')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Region of SVM using only selected features')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow, even if we choose good initial values  for beta and b that we have from the LDA, the SVM doesn't perform as expected. No matter how the hyperparameters are changed, the error rate always seems to converge to about 50%. In the Decision Region it is good visible that the reason for the ca. 50% error rate is that the decision boundary got moved so far to one side that almost every data point is on the same side. In comparison the sklearn implementation of the SVM works very well with an average accuracy of 0.986. This would suggest that either we did something wrong with the hyperparameters or our implementation is incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "The Decision Regions is again basically the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC(kernel='linear', C=1.0, gamma='auto')\n",
    "scores = cross_val_score(estimator, X_all, y_all, cv=10) \n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "\n",
    "Similar as we did it for the LDA before, we are doing the cross-validation with the sklearn implementation of the SVM and cross_val_score(). We also experimented with the hyperparameters a bit but since we didn't receive any good results there was no point in keeping our tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
