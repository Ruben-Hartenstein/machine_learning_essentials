{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy # matrices and multi - dimensional arrays , linear algebra\n",
    "import sklearn # machine learning\n",
    "import matplotlib # plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.keys())\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = images[3]\n",
    "\n",
    "assert 2 == len(img.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.gray()\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(img, interpolation='bicubic') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_indices = np.where(np.isin(target, [3, 9]))\n",
    "\n",
    "# Use the filtered indices to get the corresponding elements from data, images, and target\n",
    "data = data[filter_indices]\n",
    "images = images[filter_indices]\n",
    "target = target[filter_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_all = data\n",
    "y_all = target\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data, target, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features2d(x):\n",
    "    features = np.zeros((x.shape[0], 2))\n",
    "    for i in range(x.shape[0]):\n",
    "        features[i, 0] = x[i, 10] + x[i, 18] + x[i, 26]\n",
    "        features[i, 1] = x[i, 26] + x[i, 27] + x[i, 28]\n",
    "       \n",
    "    return features\n",
    "\n",
    "features_all = features2d(X_all)\n",
    "features_train = features2d(X_train)\n",
    "features_test = features2d(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_with_colors(points, labels):\n",
    "    # Extract x and y coordinates\n",
    "    x_coords = points[:, 0]\n",
    "    y_coords = points[:, 1]\n",
    "    \n",
    "    # Create a mask for points labeled as 3 and 9\n",
    "    mask_3 = (labels == 3)\n",
    "    mask_9 = (labels == 9)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x_coords[mask_3], y_coords[mask_3], marker='o', color='blue', label='3', alpha=0.5)\n",
    "    plt.scatter(x_coords[mask_9], y_coords[mask_9], marker='o', color='red', label='9', alpha=0.5)\n",
    "    plt.title('Scatter Plot of Training Features')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_points_with_colors(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_mean(X_train, y_train, X_test):\n",
    "\n",
    "    #Compute the mean of each class\n",
    "    mean_3 = np.mean(X_train[y_train == 3], axis=0)\n",
    "    mean_9 = np.mean(X_train[y_train == 9], axis=0)\n",
    "    \n",
    "    # Compute the Euclidean distance from each test point to the mean of each class\n",
    "    distance_3 = np.linalg.norm(X_test - mean_3, axis=1)\n",
    "    distance_9 = np.linalg.norm(X_test - mean_9, axis=1)\n",
    "    \n",
    "    # Classify test points based on the minimum distance\n",
    "    test_labels = np.where(distance_3 < distance_9, -1, 1)\n",
    "    \n",
    "    return test_labels, mean_3, mean_9\n",
    "\n",
    "def calculate_error_rate(predicted_labels, true_labels):\n",
    "    total_sample = len(true_labels)\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    for i in range(total_sample):\n",
    "        if predicted_labels[i] != -1 and true_labels[i] == 3:\n",
    "            fn += 1\n",
    "        elif predicted_labels[i] != 1 and true_labels[i] == 9:\n",
    "            fp += 1\n",
    "    \n",
    "    return (fp + fn) / total_sample  \n",
    "\n",
    "predicted_labels_mean, mean_3, mean_9 = nearest_mean(features_train, y_train, features_test)\n",
    "error_rate_mean = calculate_error_rate(predicted_labels_mean, y_test)\n",
    "print(f\"Error Rate Nearest Mean: {error_rate_mean*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 200x200 grid\n",
    "x_min, x_max = 0, 50\n",
    "y_min, y_max = 0, 50\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "grid_values = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "distance_3 = np.linalg.norm(grid_values - mean_3, axis=1)\n",
    "distance_9 = np.linalg.norm(grid_values - mean_9, axis=1)\n",
    "    \n",
    "# Classify test points based on the minimum distance\n",
    "decision_region = np.where(distance_3 < distance_9, -1, 1)\n",
    "\n",
    "# Extract x and y coordinates from grid_values\n",
    "x_coords = grid_values[:, 0]\n",
    "y_coords = grid_values[:, 1]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_coords, y_coords, c=np.where(decision_region == -1, 'blue', 'red'), marker='o', alpha=0.5)\n",
    "plt.scatter(features_test[y_test == 3, 0], features_test[y_test == 3, 1], c='green', marker='o', label='3')\n",
    "plt.scatter(features_test[y_test == 9, 0], features_test[y_test == 9, 1], c='yellow', marker='o', label='9')\n",
    "plt.scatter(mean_3[0], mean_3[1], c='black', marker='o', label='Mean 3', linewidth=4)\n",
    "plt.scatter(mean_9[0], mean_9[1], c='black', marker='*', label='Mean 9', linewidth=4)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Region of Nearest Mean Classifier')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lda(X_train, y_train):\n",
    "    # Compute the mean of each class\n",
    "    mu_3 = np.mean(X_train[y_train == 3], axis=0)\n",
    "    mu_9 = np.mean(X_train[y_train == 9], axis=0)\n",
    "    mu = np.array([mu_3, mu_9])\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    cov = 0\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        if y == 3:\n",
    "            x_i = x - mu_3\n",
    "        else:    \n",
    "            x_i = x - mu_9\n",
    "        cov += np.outer(x_i, np.transpose(x_i))\n",
    "\n",
    "    covmat = cov/len(y_train)\n",
    "    # Compute the prior probability of each class\n",
    "    p = np.array([np.sum(y_train == 3) / len(y_train), np.sum(y_train == 9) / len(y_train)])\n",
    "\n",
    "    return mu, covmat, p\n",
    "\n",
    "def filter_pixels(X_train, X_test, var=0.001):\n",
    "    pixel_variances = np.var(X_train, axis=0)\n",
    "    # Find pixels with variance smaller than 0.001\n",
    "    dead_pixels_mask = pixel_variances < var\n",
    "    # Filter out dead pixels\n",
    "    X_train_filtered = X_train[:, ~dead_pixels_mask]\n",
    "    X_test_filtered = X_test[:, ~dead_pixels_mask]\n",
    "    return X_train_filtered, X_test_filtered\n",
    "\n",
    "X_train_filtered, X_test_filtered = filter_pixels(X_train, X_test)\n",
    "feature_mu, feature_covmat, feature_p = fit_lda(features_train, y_train)\n",
    "var_mu, var_covmat, var_p = fit_lda(X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lda(mu, covmat, p, test_features):\n",
    "    # Compute the inverse of the covariance matrix\n",
    "    covmat_inv = np.linalg.inv(covmat)\n",
    "\n",
    "    beta = np.dot(covmat_inv, np.transpose(mu[1] - mu[0]))\n",
    "    b = -0.5 * np.dot((mu[0] + mu[1]), beta) + np.log(p[1] / p[0])\n",
    "    # Compute decision function scores\n",
    "    decision_scores = np.dot(test_features, beta) + b\n",
    "    # Classify test points based on the decision function scores\n",
    "    predicted_labels = np.sign(decision_scores)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "predicted_labels_feature_test = predict_lda(feature_mu, feature_covmat, feature_p, features_test)   \n",
    "predicted_labels_var_test = predict_lda(var_mu, var_covmat, var_p, X_test_filtered)\n",
    "predicted_labels_feature_train = predict_lda(feature_mu, feature_covmat, feature_p, features_train)   \n",
    "predicted_labels_var_train = predict_lda(var_mu, var_covmat, var_p, X_train_filtered)\n",
    "\n",
    "error_rate_lda_feature_test = calculate_error_rate(predicted_labels_feature_test, y_test)\n",
    "error_rate_lda_var_test = calculate_error_rate(predicted_labels_var_test, y_test)\n",
    "error_rate_lda_feature_train = calculate_error_rate(predicted_labels_feature_train, y_train)\n",
    "error_rate_lda_var_train = calculate_error_rate(predicted_labels_var_train, y_train)\n",
    "\n",
    "print(f\"Error Rate LDA Feature Test: {error_rate_lda_feature_test*100:.2f}%\")\n",
    "print(f\"Error Rate LDA Variance Test: {error_rate_lda_var_test*100:.2f}%\")\n",
    "print(f\"Error Rate LDA Feature Train: {error_rate_lda_feature_train*100:.2f}%\")\n",
    "print(f\"Error Rate LDA Variance Train: {error_rate_lda_var_train*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the distinct error rates obtained from our implementation of LDA. The four values represent various combinations of LDA error rates: one with our handcrafted features and another with pixel data exhibiting sufficiently high variance, each tested using our separate test and train datasets. This data suggests that our handcrafted features are not very good in differentiating between 3's and 9's. Another noticeable characteristic is that the Error rate for our features is higher for the Train data then it is for the Test data. This is normally not the case but it could be a coincidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "features = [features_train, features_test]\n",
    "y = [y_train, y_test]\n",
    "titles = [\"Decision Region of LDA Training Data\", \"Decision Region of LDA Test Data\"]\n",
    "for i in range(len(features)):\n",
    "\n",
    "    # Create a 200x200 grid\n",
    "    x_min, x_max = 0, 50\n",
    "    y_min, y_max = 0, 50\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    grid_values = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Classify test points based on the decision boundary\n",
    "    decision_region = predict_lda(feature_mu, feature_covmat, feature_p, grid_values) \n",
    "\n",
    "    # Extract x and y coordinates from grid_values\n",
    "    x_coords = grid_values[:, 0]\n",
    "    y_coords = grid_values[:, 1]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x_coords, y_coords, c=np.where(decision_region == -1, 'blue', 'red'), marker='o', alpha=0.5)\n",
    "\n",
    "    plt.scatter(features[i][y[i] == 3, 0], features[i][y[i] == 3, 1], c='green', marker='o', label='3')\n",
    "    plt.scatter(features[i][y[i] == 9, 0], features[i][y[i] == 9, 1], c='yellow', marker='o', label='9')\n",
    "\n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(feature_covmat)\n",
    "\n",
    "    # Plot each Gaussian distribution\n",
    "    for j in range(len(feature_mu)):\n",
    "        # Compute PDF values for the grid points\n",
    "        pdf_values = multivariate_normal.pdf(grid_values, mean=feature_mu[j], cov=feature_covmat)\n",
    "\n",
    "        # Reshape PDF values to match grid shape\n",
    "        pdf_values = pdf_values.reshape(xx.shape)\n",
    "\n",
    "        # Plot isocontours (ellipses)\n",
    "        plt.contour(xx, yy, pdf_values, colors='k')\n",
    "\n",
    "        # Plot mean of the Gaussian distribution\n",
    "        plt.plot(feature_mu[j][0], feature_mu[j][1], 'ko')\n",
    "\n",
    "        # Compute standard deviations along each principal component (square roots of eigenvalues)\n",
    "        std_devs = np.sqrt(eigenvalues)\n",
    "\n",
    "        # Get principal cluster axes (eigenvectors)\n",
    "        principal_axes = eigenvectors.T[j] * std_devs[j]  # Scale eigenvectors by standard deviations\n",
    "\n",
    "        # Plot principal cluster axes\n",
    "        plt.plot([feature_mu[j][0], feature_mu[j][0] + principal_axes[0]], [feature_mu[j][1], feature_mu[j][1] + principal_axes[1]], 'k--', linewidth=3)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(titles[i])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the points for the LDA with the Training data look like they are closer to the mean of the gaussian distribution, the actual error rate is higher for the LDA with the Training data which is unusual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "estimator = LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "scores = cross_val_score(estimator, X_all, y_all, cv=10)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn implementation of the LDA reaches even for all 64 pixels used as feature a pretty high accuracy of 0.978. This, as well as the different Error Rates from our implementation would suggest that our features are not very well selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta_loss(X, y, beta, b, _lambda):\n",
    "    sum_b = 0\n",
    "    sum_beta = 0\n",
    "    for i in range(len(y)):\n",
    "            if y[i] * (np.dot(X[i], beta) + b) < 1:\n",
    "                sum_beta -= y[i] * np.transpose(X[i])\n",
    "                sum_b -= y[i]\n",
    "    delta_b = (_lambda / len(y)) * sum_b\n",
    "    delta_beta = beta + (_lambda / len(y)) * sum_beta\n",
    "\n",
    "    return delta_beta, delta_b\n",
    "\n",
    "def fit_svm(training_features, training_labels, _lambda, beta, b, tau):\n",
    "    _sum = 0\n",
    "    for i in range(len(training_labels)):\n",
    "        _sum += np.maximum(1 - training_labels[i] * (np.dot(training_features[i], beta)) + b, 0)\n",
    "\n",
    "    loss = 0.5 * np.dot(np.transpose(beta), beta) + (_lambda / len(training_labels)) * _sum\n",
    "\n",
    "    delta_beta, delta_b = calculate_delta_loss(training_features, training_labels, beta, b, _lambda)\n",
    "    beta -= tau * delta_beta\n",
    "    b -= tau * delta_b\n",
    "\n",
    "    return loss, beta, b\n",
    "\n",
    "def predict_svm(test_features, beta, b):\n",
    "    return np.sign(np.dot(test_features, beta) + b)\n",
    "\n",
    "\n",
    "tau = 0.0001\n",
    "_lambda = 10\n",
    "#beta = np.random.normal(0, 1, training_features.shape[1])\n",
    "#b = 0\n",
    "beta = np.array([0.24034121, 0.08780151])\n",
    "b = -8.948143239056721\n",
    "\n",
    "epochs = 1000\n",
    "error_rate_list = []\n",
    "loss_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i == 900:\n",
    "        tau /= 10\n",
    "    predicted_labels = predict_svm(features_test, beta, b)\n",
    "    error_rate = calculate_error_rate(predicted_labels, y_test)\n",
    "    loss, beta, b = fit_svm(features_train, y_train, _lambda, beta, b, tau)\n",
    "    error_rate_list.append(error_rate*100)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "plt.plot(range(epochs), error_rate_list, label='Error Rate')\n",
    "plt.plot(range(epochs), loss_list, label='Loss')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Error Rate and Loss vs Iteration')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 200x200 grid\n",
    "x_min, x_max = 0, 50\n",
    "y_min, y_max = 0, 50\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "grid_values = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Classify test points based on the decision boundary\n",
    "decision_region = predict_svm(grid_values, beta, b)\n",
    "\n",
    "# Extract x and y coordinates from grid_values\n",
    "x_coords = grid_values[:, 0]\n",
    "y_coords = grid_values[:, 1]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_coords, y_coords, c=np.where(decision_region == -1, 'blue', 'red'), marker='o', alpha=0.5)\n",
    "plt.scatter(features_train[:, 0], features_train[:, 1], c=np.where(y_train == 3, 'green', 'yellow'), marker='o', label='Training Data')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Region of SVM using only selected features')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow, even if we choose good initial values  for beta and b that we have from the LDA, the SVM doesn't perform as expected. No matter how the hyperparameters are changed, the error rate always seems to converge to about 50%. In the Decision Region it is good visible that the reason for the ca. 50% error rate is that the decision boundary got moved so far to one side that almost every data point is on the same side. In comparison the sklearn implementation of the SVM works very well with an average accuracy of 0.986. This would suggest that either we did something wrong with the hyperparameters or our implementation is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC(kernel='linear', C=1.0, gamma='auto')\n",
    "scores = cross_val_score(estimator, X_all, y_all, cv=10) \n",
    "\n",
    "print(scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
