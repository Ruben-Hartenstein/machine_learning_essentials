{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03: Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.distributions.binomial as binomial\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def init_prelu(shape, value=0.25):\n",
    "    return torch.full(shape, value, requires_grad=True)\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_error(shape, model, params):\n",
    "    test_error = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_dataloader):\n",
    "            x, y = batch\n",
    "            x = x.reshape(*shape)\n",
    "            \n",
    "            # Forward pass to get predictions \n",
    "            noise_py_x = model(*params)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            predicted_probs = softmax(noise_py_x, dim=1)\n",
    "            \n",
    "            # Get the predicted labels by taking the argmax of the predicted probabilities\n",
    "            predicted_labels = torch.argmax(predicted_probs, dim=1)\n",
    "            \n",
    "            # Compute the number of incorrectly classified samples in this batch\n",
    "            incorrect_count = torch.sum(predicted_labels != y)\n",
    "            \n",
    "            # Accumulate the total number of incorrectly classified samples\n",
    "            test_error += incorrect_count.item()\n",
    "\n",
    "    # Compute the average test error across all batches\n",
    "    total_samples = len(test_dataloader) * batch_size\n",
    "    test_error /= total_samples\n",
    "\n",
    "    print(f\"Test Error: {test_error * 100:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pre_softmax\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# initialize weights\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# input shape is (B, 784)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m w_h \u001b[38;5;241m=\u001b[39m \u001b[43minit_weights\u001b[49m((\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m625\u001b[39m))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# hidden layer with 625 neurons\u001b[39;00m\n\u001b[0;32m     28\u001b[0m w_h2 \u001b[38;5;241m=\u001b[39m init_weights((\u001b[38;5;241m625\u001b[39m, \u001b[38;5;241m625\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'init_weights' is not defined"
     ]
    }
   ],
   "source": [
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        mask = torch.bernoulli(torch.full_like(X, 1-p_drop))\n",
    "        return mask * X / (1.0 - p_drop)\n",
    "    return X\n",
    "\n",
    "\n",
    "def dropout_model(x, w_h, w_h2, w_o, p_drop_input=0.5, p_drop_hidden=0.5):\n",
    "    h = rectify(dropout(x @ w_h, p_drop_input))\n",
    "    h2 = rectify(dropout(h @ w_h2, p_drop_hidden))\n",
    "    pre_softmax = dropout(h2 @ w_o)\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o):\n",
    "    h = rectify(x @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = dropout_model(x, w_h, w_h2, w_o)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "calculate_test_error((batch_size, 784), model, (x, w_h, w_h2, w_o))\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Dropout method works, and how it reduces overfitting\n",
    "Dropout is a technique where randomly selected neurons are ignored during training, meaning that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. The effect is that the network becomes less sensitive to the specific weights of neurons, and therefore less likely to overfit to the training data.\n",
    "\n",
    "### Difference with and without Dropout method\n",
    "With the implemented dropout_model() function, we observe significantly less overfitting. During the first run, the test data shows a loss value of about 0.8, while the training data performs much worse in comparison. Additionally, both training and test sets exhibit decreasing performance over the epochs, suggesting good initialization but poor training behavior.\n",
    "\n",
    "### Why do we need a different model configuration for evaluating the test loss?\n",
    "When evaluating the test loss, we use the original model without dropout. This is because dropout is a regularization technique used to prevent overfitting during training, and is not typically used during testing or inference. During testing, we want to use the full capacity of the model, so we don't apply dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu(X, a):\n",
    "    return torch.max(torch.zeros_like(X), X) + torch.mul(a, torch.min(torch.zeros_like(X), X))\n",
    "\n",
    "\n",
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        mask = torch.bernoulli(torch.full_like(X, 1-p_drop))\n",
    "        return mask * X / (1.0 - p_drop)\n",
    "    return X\n",
    "\n",
    "\n",
    "def dropout_model(x, w_h, w_h2, w_o, a, a2, p_drop_input=0.5, p_drop_hidden=0.5):\n",
    "    h = PRelu(dropout(x @ w_h, p_drop_input), a)\n",
    "    h2 = PRelu(dropout(h @ w_h2, p_drop_hidden), a2)\n",
    "    pre_softmax = dropout(h2 @ w_o)\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o, a, a2):\n",
    "    h = PRelu(x @ w_h, a)\n",
    "    h2 = PRelu(h @ w_h2, a2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "# Initialize PReLU parameter with Gaussian distribution\n",
    "a = init_prelu((1, w_h.shape[1]))\n",
    "a2 = init_prelu((1, w_h2.shape[1]))\n",
    "\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o, a, a2])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = dropout_model(x, w_h, w_h2, w_o, a, a2)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o, a, a2)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "calculate_test_error((batch_size, 784), model, (x, w_h, w_h2, w_o, a, a2))\n",
    "\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Relu and PRelu\n",
    "We initialized a as proposed by Kaiming He et al. with 0.25. With the use of the Parametric Rectified Linear Unit we are observing essential improvment in the loss rate of our test data as we are approaching a loss of 0.2 as opposed of 0.8 using Relu. The model has also achieved better results for the train data with a loss of 1.2 instead of a loss of 2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 4.84e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 10\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 20\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 30\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 40\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 50\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 60\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 70\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 80\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 90\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 100\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21e29f063d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABB+ElEQVR4nO3deXxTVf7/8XeStmlpaQsILWWtgixCkU0poKCgIKgsLsjgsIygIij8lPmO6KgsYhkdYXRQFlFwwUFgZFFRRAQdFWQREFBxAaEKpSi0pSIFmvP7oyYl0JbS3uTS9vV8PPKQ3Jzce3KayptzP+dehzHGCAAAoJxw2t0BAAAAKxFuAABAuUK4AQAA5QrhBgAAlCuEGwAAUK4QbgAAQLlCuAEAAOUK4QYAAJQrhBsAAFCuEG6AIgwePFj169e3uxsl0rlzZ3Xu3NnubqACKc13riz/ruH8Q7hBmeRwOIr1WLNmjd1dPW+NGzeuWGNoVUBavny5xo0bV+z2nTt3VrNmzSw5dkX2448/Fvv35ccff7S7u4AlQuzuAFASr776qt/zV155RStXrjxje5MmTUp1nBdeeEEej6dU+zhf9e3bVw0aNPA9z87O1vDhw9WnTx/17dvXtz0uLs6S4y1fvlzPPffcOQUclF716tXP+L14+umn9dNPP2nq1KlntC2N999/v8TvLc+/awg+wg3KpNtvv93v+bp167Ry5coztp/u6NGjqlSpUrGPExoaWqL+lQVJSUlKSkryPf/ll180fPhwJSUlnXUccX4q6PsdGRl5xs9z/vz5Onz4cJE/Z2OMjh07poiIiGIfPyws7Nw6fIry/LuG4OO0FMot72mNTZs26corr1SlSpX00EMPSZKWLl2qnj17KiEhQW63WxdddJEmTpyo3Nxcv32cXgfgneL/5z//qVmzZumiiy6S2+1W27ZttWHDhrP26dChQxozZoyaN2+uqKgoRUdH67rrrtPWrVv92q1Zs0YOh0MLFizQpEmTVLt2bYWHh6tLly76/vvvz9ivty8RERG67LLL9L///a8EI1awb775RjfffLOqVq2q8PBwtWnTRsuWLfNrc+LECY0fP14NGzZUeHi4qlWrpo4dO2rlypWS8sbxueeek+R/StEKzz//vC655BK53W4lJCRoxIgRysjI8Gvz3Xff6aabblJ8fLzCw8NVu3Zt3XbbbcrMzPS1WblypTp27KjY2FhFRUWpUaNGvu9LUU6ePKmJEyf6vgv169fXQw89pJycHF+b66+/XhdeeGGB709OTlabNm38tr322mtq3bq1IiIiVLVqVd12221KTU31a1PU97sk6tevr+uvv14rVqxQmzZtFBERoZkzZ0qS5syZo6uvvlo1atSQ2+1W06ZNNX369DP2cXrNzbl8j0v7u7Zw4UI1bdpU4eHhatasmRYvXkwdTwXGzA3KtV9//VXXXXedbrvtNt1+++2+Uyxz585VVFSU7r//fkVFRenDDz/Uo48+qqysLD311FNn3e/rr7+uI0eO6K677pLD4dCTTz6pvn37ateuXUX+C3TXrl1asmSJbrnlFiUmJurAgQOaOXOmOnXqpK+++koJCQl+7SdPniyn06kxY8YoMzNTTz75pAYMGKDPP//c1+bFF1/UXXfdpfbt22v06NHatWuXbrzxRlWtWlV16tQp4cjl2bFjhzp06KBatWrpwQcfVGRkpBYsWKDevXvrv//9r/r06SMpr34nJSVFQ4cO1WWXXaasrCxt3LhRX3zxha655hrddddd2rdvX4GnDktj3LhxGj9+vLp27arhw4dr586dmj59ujZs2KBPP/1UoaGhOn78uLp166acnBzde++9io+P188//6y3335bGRkZiomJ0Y4dO3T99dcrKSlJEyZMkNvt1vfff69PP/30rH0YOnSoXn75Zd1888164IEH9PnnnyslJUVff/21Fi9eLEnq16+fBg4cqA0bNqht27a+9+7Zs0fr1q3z+85NmjRJjzzyiG699VYNHTpUBw8e1L///W9deeWV2rx5s2JjY31tC/t+l9TOnTvVv39/3XXXXRo2bJgaNWokSZo+fbouueQS3XjjjQoJCdFbb72le+65Rx6PRyNGjDjrfovzPS5McX7X3nnnHfXr10/NmzdXSkqKDh8+rDvuuEO1atUq1XigDDNAOTBixAhz+te5U6dORpKZMWPGGe2PHj16xra77rrLVKpUyRw7dsy3bdCgQaZevXq+57t37zaSTLVq1cyhQ4d825cuXWokmbfeeqvIfh47dszk5ub6bdu9e7dxu91mwoQJvm2rV682kkyTJk1MTk6Ob/szzzxjJJlt27YZY4w5fvy4qVGjhrn00kv92s2aNctIMp06dSqyP6c6ePCgkWQee+wx37YuXbqY5s2b+42Jx+Mx7du3Nw0bNvRta9GihenZs2eR+y/oZ1SUTp06mUsuuaTQ19PT001YWJi59tpr/cZ02rRpRpJ56aWXjDHGbN682UgyCxcuLHRfU6dONZLMwYMHi90/Y4zZsmWLkWSGDh3qt33MmDFGkvnwww+NMcZkZmYat9ttHnjgAb92Tz75pHE4HGbPnj3GGGN+/PFH43K5zKRJk/zabdu2zYSEhPhtL+r7fTY9e/b0+14bY0y9evWMJPPee++d0b6g35du3bqZCy+80G9bp06d/L5zxf0eG1O637XmzZub2rVrmyNHjvi2rVmzxkg643OiYuC0FMo1t9utIUOGnLH91DqCI0eO6JdfftEVV1yho0eP6ptvvjnrfvv166cqVar4nl9xxRWS8mZmztYfpzPv1y43N1e//vqr7xTIF198cUb7IUOG+NUxnH6cjRs3Kj09XXfffbdfu8GDBysmJuasn6Mohw4d0ocffqhbb73VN0a//PKLfv31V3Xr1k3fffedfv75Z0lSbGysduzYoe+++65UxzwXH3zwgY4fP67Ro0f7xlSShg0bpujoaL3zzjuS5BuHFStW6OjRowXuyzsbsnTp0nMqal2+fLkk6f777/fb/sADD0iSrw/e048LFiyQMcbX7o033lC7du1Ut25dSdKbb74pj8ejW2+91Tfev/zyi+Lj49WwYUOtXr3a7ziFfb9LKjExUd26dTtj+6m/L5mZmfrll1/UqVMn7dq1y+/UXmHO9j0uytl+1/bt26dt27Zp4MCBioqK8rXr1KmTmjdvftb9o3wi3KBcq1WrVoFFjjt27FCfPn0UExOj6OhoVa9e3VdcWZz/WXv/MvLy/s/38OHDRb7P4/Fo6tSpatiwodxuty644AJVr15dX375ZYHHPdtx9uzZI0lq2LChX7vQ0NBCazyK6/vvv5cxRo888oiqV6/u93jsscckSenp6ZKkCRMmKCMjQxdffLGaN2+uv/71r/ryyy9Ldfyz8X5276kTr7CwMF144YW+1xMTE3X//fdr9uzZuuCCC9StWzc999xzfuPdr18/dejQQUOHDlVcXJxuu+02LViw4KxBZ8+ePXI6nX6rziQpPj5esbGxvj54j5Gamqq1a9dKkn744Qdt2rRJ/fr187X57rvvZIxRw4YNzxjzr7/+2jfeXoV9v0sqMTGxwO2ffvqpunbtqsjISMXGxqp69eq++p5A/r4U573eMT79Z1DYNlQM1NygXCtopUdGRoY6deqk6OhoTZgwQRdddJHCw8P1xRdf6G9/+1ux/uXucrkK3H7qv8oL8sQTT+iRRx7RX/7yF02cOFFVq1aV0+nU6NGjCzxuSY9jBW9/xowZU+C/5qX8vzyuvPJK/fDDD1q6dKnef/99zZ49W1OnTtWMGTM0dOjQgPf1bJ5++mkNHjzY17/77rtPKSkpWrdunWrXrq2IiAh9/PHHWr16td555x299957euONN3T11Vfr/fffL/Tn4FWc4ugbbrhBlSpV0oIFC9S+fXstWLBATqdTt9xyi6+Nx+ORw+HQu+++W+AxT52ZkAr+fpdGQfv74Ycf1KVLFzVu3FhTpkxRnTp1FBYWpuXLl2vq1KkB/X0p7XtRcRFuUOGsWbNGv/76q958801deeWVvu27d+8O+LEXLVqkq666Si+++KLf9oyMDF1wwQXnvL969epJyvsX/9VXX+3bfuLECe3evVstWrQocV+9Mz+hoaHq2rXrWdtXrVpVQ4YM0ZAhQ5Sdna0rr7xS48aN84Ubq1ZHeXk/+86dO/1mqY4fP67du3ef0efmzZurefPm+vvf/67PPvtMHTp00IwZM/T4449LkpxOp7p06aIuXbpoypQpeuKJJ/Twww9r9erVhX7+evXqyePx6LvvvvO7ptKBAweUkZHh66OUtyT7+uuv18KFCzVlyhS98cYbuuKKK/yKyC+66CIZY5SYmKiLL7649INkgbfeeks5OTlatmyZ3yzK6afI7OId44JWERa0DRUDp6VQ4Xj/JXjqv/yOHz+u559/PijHPv1fnAsXLvTVrpyrNm3aqHr16poxY4aOHz/u2z537twzlkOfqxo1aqhz586aOXOm9u/ff8brBw8e9P35119/9XstKipKDRo08FsOHRkZKUml7pdX165dFRYWpmeffdZvTF988UVlZmaqZ8+ekqSsrCydPHnS773NmzeX0+n09e/QoUNn7P/SSy+VJL/PcLoePXpIkv71r3/5bZ8yZYok+frg1a9fP+3bt0+zZ8/W1q1b/U5JSXkXVnS5XBo/fvwZ3xNjzBnjHAwF/b5kZmZqzpw5Qe9LQRISEtSsWTO98sorys7O9m3/6KOPtG3bNht7Bjsxc4MKp3379qpSpYoGDRqk++67Tw6HQ6+++mpQprmvv/56TZgwQUOGDFH79u21bds2zZs3r8T1MaGhoXr88cd111136eqrr1a/fv20e/duzZkzp9Q1N5L03HPPqWPHjmrevLmGDRumCy+8UAcOHNDatWv1008/+a7P07RpU3Xu3FmtW7dW1apVtXHjRi1atEgjR4707at169aSpPvuu0/dunWTy+XSbbfdVuTxDx486JtZOVViYqIGDBigsWPHavz48erevbtuvPFG7dy5U88//7zatm3rq6H68MMPNXLkSN1yyy26+OKLdfLkSb366qtyuVy66aabJOXVDH388cfq2bOn6tWrp/T0dD3//POqXbu2OnbsWGj/WrRooUGDBmnWrFm+053r16/Xyy+/rN69e+uqq67ya9+jRw9VrlxZY8aM8Tu+10UXXaTHH39cY8eO1Y8//qjevXurcuXK2r17txYvXqw777xTY8aMKXLMrHbttdcqLCxMN9xwg+666y5lZ2frhRdeUI0aNQoMvXZ44okn1KtXL3Xo0EFDhgzR4cOHNW3aNDVr1swv8KACsWGFFmC5wpaCF7aU+NNPPzXt2rUzERERJiEhwfzf//2fWbFihZFkVq9e7WtX2PLUp5566ox96rRl1AU5duyYeeCBB0zNmjVNRESE6dChg1m7dm2hS2hPX77sPf6cOXP8tj///PMmMTHRuN1u06ZNG/Pxxx+fsc+zKWgpuDHG/PDDD2bgwIEmPj7ehIaGmlq1apnrr7/eLFq0yNfm8ccfN5dddpmJjY01ERERpnHjxmbSpEnm+PHjvjYnT5409957r6levbpxOBxnXRbuXepc0KNLly6+dtOmTTONGzc2oaGhJi4uzgwfPtwcPnzY9/quXbvMX/7yF3PRRReZ8PBwU7VqVXPVVVeZDz74wNdm1apVplevXiYhIcGEhYWZhIQE079/f/Ptt9+eddxOnDhhxo8fbxITE01oaKipU6eOGTt2rN/y+VMNGDDASDJdu3YtdJ///e9/TceOHU1kZKSJjIw0jRs3NiNGjDA7d+70G5+ilsoXpbCl4IUt51+2bJlJSkoy4eHhpn79+uYf//iHeemll4wks3v3br8+lfR7XNrftfnz55vGjRsbt9ttmjVrZpYtW2Zuuukm07hx4yLHAuWTwxiqsgAA5c+ll16q6tWr+66UjYqDmhsAQJl24sSJM+qq1qxZo61bt1p2V3uULczcAADKtB9//FFdu3bV7bffroSEBH3zzTeaMWOGYmJitH37dlWrVs3uLiLIKCgGAJRpVapUUevWrTV79mwdPHhQkZGR6tmzpyZPnkywqaCYuQEAAOUKNTcAAKBcIdwAAIBypcLV3Hg8Hu3bt0+VK1e2/HLwAAAgMIwxOnLkiBISEuR0Fj03U+HCzb59+1SnTh27uwEAAEogNTVVtWvXLrJNhQs3lStXlpQ3ONHR0Tb3BgAAFEdWVpbq1Knj+3u8KBUu3HhPRUVHRxNuAAAoY4pTUkJBMQAAKFcINwAAoFwh3AAAgHKlwtXcAAAQCB6PR8ePH7e7G2VWaGioXC6XJfsi3AAAUErHjx/X7t275fF47O5KmRYbG6v4+PhSX4eOcAMAQCkYY7R//365XC7VqVPnrBeYw5mMMTp69KjS09MlSTVr1izV/gg3AACUwsmTJ3X06FElJCSoUqVKdnenzIqIiJAkpaenq0aNGqU6RUW8BACgFHJzcyVJYWFhNvek7POGwxMnTpRqP4QbAAAswP0KS8+qMSTcAACAcoVwAwAALFG/fn3961//srsbhBsAACoah8NR5GPcuHEl2u+GDRt05513WtvZEmC1lEV+P56rX3/LUViIUzUqh9vdHQAACrV//37fn9944w09+uij2rlzp29bVFSU78/GGOXm5iok5OyRoXr16tZ2tISYubHI+1+lqeM/Vuv/vbHF7q4AAFCk+Ph43yMmJkYOh8P3/JtvvlHlypX17rvvqnXr1nK73frkk0/0ww8/qFevXoqLi1NUVJTatm2rDz74wG+/p5+Wcjgcmj17tvr06aNKlSqpYcOGWrZsWcA/H+HGIi5nXoV3rsfY3BMAgJ2MMTp6/KQtD2Os+zvowQcf1OTJk/X1118rKSlJ2dnZ6tGjh1atWqXNmzere/fuuuGGG7R3794i9zN+/Hjdeuut+vLLL9WjRw8NGDBAhw4dsqyfBeG0lEVcDsINAED6/USumj66wpZjfzWhmyqFWfNX+4QJE3TNNdf4nletWlUtWrTwPZ84caIWL16sZcuWaeTIkYXuZ/Dgwerfv78k6YknntCzzz6r9evXq3v37pb0syDM3FiEmRsAQHnSpk0bv+fZ2dkaM2aMmjRpotjYWEVFRenrr78+68xNUlKS78+RkZGKjo723WYhUJi5sQjhBgAgSRGhLn01oZttx7ZKZGSk3/MxY8Zo5cqV+uc//6kGDRooIiJCN99881nvhB4aGur33OFwBPwGo4Qbizi94cbC850AgLLH4XBYdmrofPLpp59q8ODB6tOnj6S8mZwff/zR3k4VgtNSFgnxzdzY3BEAAAKgYcOGevPNN7VlyxZt3bpVf/rTnwI+A1NShBuL5BcUn58/aAAASmPKlCmqUqWK2rdvrxtuuEHdunVTq1at7O5WgRzGynVjZUBWVpZiYmKUmZmp6Ohoy/b7+a5f1W/WOl1UPVKrHuhs2X4BAOe3Y8eOaffu3UpMTFR4OBdxLY2ixvJc/v5m5sYiFBQDAHB+INxYhIJiAADOD4Qbi3gLiim5AQDAXoQbizj/KCg+SboBAMBWhBuLhLhYCg4AwPmAcGMRloIDAHB+INxYxMlqKQAAzguEG4uEEG4AADgvEG4s4i0oZik4AAD2ItxYxFtQTMkNAAD2ItxYxMVScABAGeFwOIp8jBs3rlT7XrJkiWV9LYnyd092m3hvv+AxkjFGjj/CDgAA55v9+/f7/vzGG2/o0Ucf1c6dO33boqKi7OiWZZi5sYg33EgUFQMAzm/x8fG+R0xMjBwOh9+2+fPnq0mTJgoPD1fjxo31/PPP+957/PhxjRw5UjVr1lR4eLjq1aunlJQUSVL9+vUlSX369JHD4fA9DzZmbiziPDXcGMPAAkBFZYx04qg9xw6tJJXyzMG8efP06KOPatq0aWrZsqU2b96sYcOGKTIyUoMGDdKzzz6rZcuWacGCBapbt65SU1OVmpoqSdqwYYNq1KihOXPmqHv37nK5XFZ8qnPG38EWCTkl3FB2AwAV2Imj0hMJ9hz7oX1SWGSpdvHYY4/p6aefVt++fSVJiYmJ+uqrrzRz5kwNGjRIe/fuVcOGDdWxY0c5HA7Vq1fP997q1atLkmJjYxUfH1+qfpQG4cYizlOScl5RsT1pFQCAkvrtt9/0ww8/6I477tCwYcN820+ePKmYmBhJ0uDBg3XNNdeoUaNG6t69u66//npde+21dnW5QIQbizBzAwCQlHdq6KF99h27FLKzsyVJL7zwgi6//HK/17ynmFq1aqXdu3fr3Xff1QcffKBbb71VXbt21aJFi0p1bCsRbixyakExy8EBoAJzOEp9asgucXFxSkhI0K5duzRgwIBC20VHR6tfv37q16+fbr75ZnXv3l2HDh1S1apVFRoaqtzc3CD2+kyEG4vkXRsgr46MqxQDAMqq8ePH67777lNMTIy6d++unJwcbdy4UYcPH9b999+vKVOmqGbNmmrZsqWcTqcWLlyo+Ph4xcbGSspbMbVq1Sp16NBBbrdbVapUCfpnYCm4hbi/FACgrBs6dKhmz56tOXPmqHnz5urUqZPmzp2rxMRESVLlypX15JNPqk2bNmrbtq1+/PFHLV++XE5nXqR4+umntXLlStWpU0ctW7a05TM4jKlY0wxZWVmKiYlRZmamoqOjLd13o7+/q5yTHn3yt6tUu0rpznsCAMqGY8eOaffu3UpMTFR4eLjd3SnTihrLc/n7m5kbC3lnbii5AQDAPoQbC3kv5EdBMQAA9iHcWCj//lIV6kwfAADnFcKNhUJ8MzeEGwAA7EK4sZD3KsWslgKAiqeCrc8JCKvGkHBjIQqKAaDi8V659/jx4zb3pOw7ejTvhqOhoaGl2g8X8bMQBcUAUPGEhISoUqVKOnjwoEJDQ33Xe0HxGWN09OhRpaenKzY2ttR3EyfcWCiEgmIAqHAcDodq1qyp3bt3a8+ePXZ3p0yz6m7ihBsL+WZucgk3AFCRhIWFqWHDhpyaKoXQ0NBSz9h4EW4s5PIWFDNzAwAVjtPp5ArF5wlODFrIRUExAAC2I9xYyEVBMQAAtjtvws3kyZPlcDg0evToQtvMnTtXDofD73E+TQFSUAwAgP3Oi5qbDRs2aObMmUpKSjpr2+joaO3cudP33PFHncv5gIJiAADsZ/vMTXZ2tgYMGKAXXnhBVapUOWt7h8Oh+Ph43yMuLi4IvSweb0ExMzcAANjH9nAzYsQI9ezZU127di1W++zsbNWrV0916tRRr169tGPHjiLb5+TkKCsry+8RKC7uLQUAgO1sDTfz58/XF198oZSUlGK1b9SokV566SUtXbpUr732mjwej9q3b6+ffvqp0PekpKQoJibG96hTp45V3T+DN9xwbykAAOxjW7hJTU3VqFGjNG/evGIXBScnJ2vgwIG69NJL1alTJ7355puqXr26Zs6cWeh7xo4dq8zMTN8jNTXVqo9wBhcFxQAA2M62guJNmzYpPT1drVq18m3Lzc3Vxx9/rGnTpiknJ+esVyoMDQ1Vy5Yt9f333xfaxu12y+12W9bvorgoKAYAwHa2hZsuXbpo27ZtftuGDBmixo0b629/+1uxLsGcm5urbdu2qUePHoHq5jlhKTgAAPazLdxUrlxZzZo189sWGRmpatWq+bYPHDhQtWrV8tXkTJgwQe3atVODBg2UkZGhp556Snv27NHQoUOD3v+COB0UFAMAYLfz4jo3hdm7d6/freMPHz6sYcOGKS0tTVWqVFHr1q312WefqWnTpjb2Ml/+7RcINwAA2OW8Cjdr1qwp8vnUqVM1derU4HXoHLFaCgAA+9l+nZvyhOvcAABgP8KNhVgKDgCA/Qg3FnJRUAwAgO0INxaioBgAAPsRbiyUX1Bsc0cAAKjACDcWyg83pBsAAOxCuLGQL9xQUAwAgG0INxaioBgAAPsRbixEQTEAAPYj3FiIi/gBAGA/wo2FmLkBAMB+hBsLUVAMAID9CDcW8hYUc+NMAADsQ7ixkMtFuAEAwG6EGwuxFBwAAPsRbixEQTEAAPYj3Fgov6DY5o4AAFCBEW4sxL2lAACwH+HGQvnhhqkbAADsQrixEEvBAQCwH+HGQk5mbgAAsB3hxkIh3FsKAADbEW4s5FsKzu0XAACwDeHGQhQUAwBgP8KNhSgoBgDAfoQbC1FQDACA/Qg3Fgoh3AAAYDvCjYV8MzcUFAMAYBvCjYXyZ25s7ggAABUY4cZC+QXFpBsAAOxCuLEQS8EBALAf4cZChBsAAOxHuLEQBcUAANiPcGMhb0ExJTcAANiHcGMhp8N740zSDQAAdiHcWCjExVJwAADsRrixEEvBAQCwH+HGQtxbCgAA+xFuLMS9pQAAsB/hxkLegmKWggMAYB/CjYW8BcWU3AAAYB/CjYVcLAUHAMB2hBsLeW+/4DGS4dQUAAC2INxYyBtuJIqKAQCwC+HGQs5Tww0zNwAA2IJwY6GQU8INZTcAANiDcGMh71JwiaJiAADsQrixEDM3AADYj3BjoVMLipm5AQDAHoQbCzkcDnnPTFFQDACAPQg3FvOemmLiBgAAexBuLObkKsUAANiKcGMxZm4AALAX4cZi3gv5MXMDAIA9CDcWy7+/FAXFAADY4bwJN5MnT5bD4dDo0aOLbLdw4UI1btxY4eHhat68uZYvXx6cDhZTiG/mhnADAIAdzotws2HDBs2cOVNJSUlFtvvss8/Uv39/3XHHHdq8ebN69+6t3r17a/v27UHq6dl5C4q5cSYAAPawPdxkZ2drwIABeuGFF1SlSpUi2z7zzDPq3r27/vrXv6pJkyaaOHGiWrVqpWnTpgWpt2dHQTEAAPayPdyMGDFCPXv2VNeuXc/adu3atWe069atm9auXVvoe3JycpSVleX3CCQKigEAsFeInQefP3++vvjiC23YsKFY7dPS0hQXF+e3LS4uTmlpaYW+JyUlRePHjy9VP89FCAXFAADYyraZm9TUVI0aNUrz5s1TeHh4wI4zduxYZWZm+h6pqakBO5Z0ysxNLuEGAAA72DZzs2nTJqWnp6tVq1a+bbm5ufr44481bdo05eTkyOVy+b0nPj5eBw4c8Nt24MABxcfHF3oct9stt9ttbeeL4PIWFDNzAwCALWybuenSpYu2bdumLVu2+B5t2rTRgAEDtGXLljOCjSQlJydr1apVfttWrlyp5OTkYHX7rFwUFAMAYCvbZm4qV66sZs2a+W2LjIxUtWrVfNsHDhyoWrVqKSUlRZI0atQoderUSU8//bR69uyp+fPna+PGjZo1a1bQ+18YFwXFAADYyvbVUkXZu3ev9u/f73vevn17vf7665o1a5ZatGihRYsWacmSJWeEJDtRUAwAgL1sXS11ujVr1hT5XJJuueUW3XLLLcHpUAlQUAwAgL3O65mbsshbUMzMDQAA9iDcWMzFvaUAALAV4cZi3nDDvaUAALAH4cZiLgqKAQCwFeHGYi4KigEAsBXhxmIUFAMAYC/CjcUoKAYAwF6EG4vl336BcAMAgB0INxZjtRQAAPYi3FiM01IAANiLcGMxloIDAGAvwo3FvKulmLkBAMAehBuLUVAMAIC9CDcWyy8otrkjAABUUIQbi+WHG9INAAB2INxYzBduKCgGAMAWhBuLUVAMAIC9CDcWo6AYAAB7EW4sxkX8AACwF+HGYszcAABgL8KNxSgoBgDAXoQbi3kLirlxJgAA9iDcWMzlItwAAGAnwo3FWAoOAIC9CDcWo6AYAAB7EW4sll9QbHNHAACooAg3FuPeUgAA2ItwY7H8cMPUDQAAdiDcWIyl4AAA2ItwYzEnMzcAANiKcGOxEAqKAQCwFeHGYhQUAwBgL8KNxSgoBgDAXoQbi1FQDACAvQg3FqOgGAAAexFuLBZCuAEAwFaEG4v5Zm4M4QYAADsQbiyWP3Njc0cAAKigShRuUlNT9dNPP/mer1+/XqNHj9asWbMs61hZlV9QTLoBAMAOJQo3f/rTn7R69WpJUlpamq655hqtX79eDz/8sCZMmGBpB8saloIDAGCvEoWb7du367LLLpMkLViwQM2aNdNnn32mefPmae7cuVb2r8wh3AAAYK8ShZsTJ07I7XZLkj744APdeOONkqTGjRtr//791vWuDKKgGAAAe5Uo3FxyySWaMWOG/ve//2nlypXq3r27JGnfvn2qVq2apR0sa7wFxZTcAABgjxKFm3/84x+aOXOmOnfurP79+6tFixaSpGXLlvlOV1VUzj8Kik+SbgAAsEVISd7UuXNn/fLLL8rKylKVKlV82++8805VqlTJss6VRSEuloIDAGCnEs3c/P7778rJyfEFmz179uhf//qXdu7cqRo1aljawbKGpeAAANirROGmV69eeuWVVyRJGRkZuvzyy/X000+rd+/emj59uqUdLGu4txQAAPYqUbj54osvdMUVV0iSFi1apLi4OO3Zs0evvPKKnn32WUs7WNZwbykAAOxVonBz9OhRVa5cWZL0/vvvq2/fvnI6nWrXrp327NljaQfLGm9BMUvBAQCwR4nCTYMGDbRkyRKlpqZqxYoVuvbaayVJ6enpio6OtrSDZY23oJiSGwAA7FGicPPoo49qzJgxql+/vi677DIlJydLypvFadmypaUdLGtcLAUHAMBWJVoKfvPNN6tjx47av3+/7xo3ktSlSxf16dPHss6VRd6CYo+RjDFy/BF2AABAcJQo3EhSfHy84uPjfXcHr127doW/gJ+UX1As5RUVe09TAQCA4CjRaSmPx6MJEyYoJiZG9erVU7169RQbG6uJEyfKU8FPxzhPDTcUFQMAEHQlmrl5+OGH9eKLL2ry5Mnq0KGDJOmTTz7RuHHjdOzYMU2aNMnSTpYlp87cVPCcBwCALUo0c/Pyyy9r9uzZGj58uJKSkpSUlKR77rlHL7zwgubOnVvs/UyfPl1JSUmKjo5WdHS0kpOT9e677xbafu7cuXI4HH6P8PDwknyEgHGeUmNDUTEAAMFXopmbQ4cOqXHjxmdsb9y4sQ4dOlTs/dSuXVuTJ09Ww4YNZYzRyy+/rF69emnz5s265JJLCnxPdHS0du7c6Xt+vhXsMnMDAIC9SjRz06JFC02bNu2M7dOmTVNSUlKx93PDDTeoR48eatiwoS6++GJNmjRJUVFRWrduXaHvcTgcvmLm+Ph4xcXFleQjBIzLycwNAAB2KtHMzZNPPqmePXvqgw8+8F3jZu3atUpNTdXy5ctL1JHc3FwtXLhQv/32m2+fBcnOzla9evXk8XjUqlUrPfHEE4XO8khSTk6OcnJyfM+zsrJK1L/iyjtdJhlDQTEAAHYo0cxNp06d9O2336pPnz7KyMhQRkaG+vbtqx07dujVV189p31t27ZNUVFRcrvduvvuu7V48WI1bdq0wLaNGjXSSy+9pKVLl+q1116Tx+NR+/btfcvRC5KSkqKYmBjfo06dOufUv5Lwnppi4gYAgOBzGGPd9MLWrVvVqlUr5ebmFvs9x48f1969e5WZmalFixZp9uzZ+uijjwoNOKc6ceKEmjRpov79+2vixIkFtilo5qZOnTrKzMwM2K0iGv39XeWc9OiTv12l2lUqBeQYAABUJFlZWYqJiSnW398lvoifVcLCwtSgQQNJUuvWrbVhwwY988wzmjlz5lnfGxoaqpYtW+r7778vtI3b7Zbb7basv8UR4nQoR8zcAABghxKdlgokj8fjN9NSlNzcXG3btk01a9YMcK/OjfdCfhQUAwAQfLbO3IwdO1bXXXed6tatqyNHjuj111/XmjVrtGLFCknSwIEDVatWLaWkpEiSJkyYoHbt2qlBgwbKyMjQU089pT179mjo0KF2fowzuHz3l6KgGACAYDuncNO3b98iX8/IyDing6enp2vgwIHav3+/YmJilJSUpBUrVuiaa66RJO3du1dOZ/7k0uHDhzVs2DClpaWpSpUqat26tT777LNi1ecEU4hv5oZwAwBAsJ1TuImJiTnr6wMHDiz2/l588cUiX1+zZo3f86lTp2rq1KnF3r9dvFcpziXcAAAQdOcUbubMmROofpQrLAUHAMA+511BcXlAQTEAAPYh3AQABcUAANiHcBMA3nBzMpdwAwBAsBFuAsDlLShm5gYAgKAj3ASAi4JiAABsQ7gJABcFxQAA2IZwEwAhFBQDAGAbwk0AOCkoBgDANoSbAPAWFDNzAwBA8BFuAsBbc5NLyQ0AAEFHuAkACooBALAP4SYAuEIxAAD2IdwEAFcoBgDAPoSbAKCgGAAA+xBuAiC/5oZwAwBAsBFuAiD/9guEGwAAgo1wEwD5S8EJNwAABBvhJgA4LQUAgH0INwHAUnAAAOxDuAkA72opZm4AAAg+wk0AUFAMAIB9CDcBwL2lAACwD+EmAPLDDekGAIBgI9wEgC/cUFAMAEDQEW4CgIJiAADsQ7gJAAqKAQCwD+EmALiIHwAA9iHcBAAzNwAA2IdwEwAUFAMAYB/CTQB4C4q5cSYAAMFHuAkAJ3cFBwDANoSbAAihoBgAANsQbgKAgmIAAOxDuAmA/IJimzsCAEAFRLgJAO4tBQCAfQg3AeCioBgAANsQbgKApeAAANiHcBMALAUHAMA+hJsACKGgGAAA2xBuAoCCYgAA7EO4CQAKigEAsA/hJgAoKAYAwD6EmwCgoBgAAPsQbgIghHADAIBtCDcB4Ju5MYQbAACCjXATAPkzNzZ3BACACohwEwD5BcWkGwAAgo1wEwAUFAMAYB/CTQBQUAwAgH0INwFAQTEAAPYh3ASAd+aGkhsAAIKPcBMAzj8Kik+SbgAACDrCTQCEuFgKDgCAXQg3AcBScAAA7GNruJk+fbqSkpIUHR2t6OhoJScn69133y3yPQsXLlTjxo0VHh6u5s2ba/ny5UHqbfGxFBwAAPvYGm5q166tyZMna9OmTdq4caOuvvpq9erVSzt27Ciw/Weffab+/fvrjjvu0ObNm9W7d2/17t1b27dvD3LPi+YrKCbbAAAQdA5jzq/1ylWrVtVTTz2lO+6444zX+vXrp99++01vv/22b1u7du106aWXasaMGcXaf1ZWlmJiYpSZmano6GjL+n2q1ENHdcWTqxUe6tQ3E68LyDEAAKhIzuXv7/Om5iY3N1fz58/Xb7/9puTk5ALbrF27Vl27dvXb1q1bN61duzYYXSw2b0ExJTcAAARfiN0d2LZtm5KTk3Xs2DFFRUVp8eLFatq0aYFt09LSFBcX57ctLi5OaWlphe4/JydHOTk5vudZWVnWdLwILpaCAwBgG9tnbho1aqQtW7bo888/1/DhwzVo0CB99dVXlu0/JSVFMTExvkedOnUs23dhnKfU3JxnZ/0AACj3bA83YWFhatCggVq3bq2UlBS1aNFCzzzzTIFt4+PjdeDAAb9tBw4cUHx8fKH7Hzt2rDIzM32P1NRUS/tfEG9BscSKKQAAgs32cHM6j8fjdxrpVMnJyVq1apXftpUrVxZaoyNJbrfbt9Tc+wg056nhhpkbAACCytaam7Fjx+q6665T3bp1deTIEb3++utas2aNVqxYIUkaOHCgatWqpZSUFEnSqFGj1KlTJz399NPq2bOn5s+fr40bN2rWrFl2fowznDpzQ9kNAADBZWu4SU9P18CBA7V//37FxMQoKSlJK1as0DXXXCNJ2rt3r5zO/Mml9u3b6/XXX9ff//53PfTQQ2rYsKGWLFmiZs2a2fURCuS9t5TkLSp22dcZAAAqmPPuOjeBFozr3JzM9ajBw3lXWt766LWKqRQakOMAAFBRlMnr3JQnLufpMzcAACBYCDcB4HA45D0zRUExAADBRbgJEN/9pZi4AQAgqAg3AeLkKsUAANiCcBMgzNwAAGAPwk2AeC/kx8wNAADBRbgJEJfv/lIUFAMAEEyEmwAJ8c3cEG4AAAgmwk2AeAuKuXEmAADBRbgJEAqKAQCwB+EmQCgoBgDAHoSbAKGgGAAAexBuAsQbbk7mEm4AAAgmwk2AuLwFxczcAAAQVISbAHFRUAwAgC0INwHioqAYAABbEG4CJISCYgAAbEG4CRAnBcUAANiCcBMg3oJiZm4AAAguwk2AeGtucim5AQAgqAg3AUJBMQAA9iDcBAhXKAYAwB6EmwDhCsUAANiDcBMgFBQDAGAPwk2A5NfcEG4AAAgmwk2A5N9+gXADAEAwEW4CJH8pOOEGAIBgItwECKelAACwB+EmQCgoBgDAHoSbAGHmBgAAexBuAoSCYgAA7EG4CRDuLQUAgD0INwGSH25INwAABBPhJkB84YaCYgAAgopwEyDe1VIUFAMAEFyEmwChoBgAAHsQbgKEgmIAAOxBuAkQCooBALAH4SZAKCgGAMAehJsA8RYUc+NMAACCi3ATIE7uCg4AgC0INwESwr2lAACwBeEmQFgKDgCAPQg3AZJfUGxzRwAAqGAINwHCUnAAAOxBuAkQFwXFAADYgnATICwFBwDAHoSbAGEpOAAA9iDcBEgIBcUAANiCcBMgFBQDAGAPwk2AUFAMAIA9CDcBQkExAAD2INwECAXFAADYg3ATICGEGwAAbEG4CRDfzI0h3AAAEEy2hpuUlBS1bdtWlStXVo0aNdS7d2/t3LmzyPfMnTtXDofD7xEeHh6kHhdf/syNzR0BAKCCsTXcfPTRRxoxYoTWrVunlStX6sSJE7r22mv122+/Ffm+6Oho7d+/3/fYs2dPkHpcfPkFxaQbAACCKcTOg7/33nt+z+fOnasaNWpo06ZNuvLKKwt9n8PhUHx8fKC7VyoUFAMAYI/zquYmMzNTklS1atUi22VnZ6tevXqqU6eOevXqpR07dgSje+eEgmIAAOxx3oQbj8ej0aNHq0OHDmrWrFmh7Ro1aqSXXnpJS5cu1WuvvSaPx6P27dvrp59+KrB9Tk6OsrKy/B7BQEExAAD2sPW01KlGjBih7du365NPPimyXXJyspKTk33P27dvryZNmmjmzJmaOHHiGe1TUlI0fvx4y/t7Nt6ZG0puAAAIrvNi5mbkyJF6++23tXr1atWuXfuc3hsaGqqWLVvq+++/L/D1sWPHKjMz0/dITU21ostn5fyjoPgk6QYAgKCydebGGKN7771Xixcv1po1a5SYmHjO+8jNzdW2bdvUo0ePAl93u91yu92l7eo5C3GxFBwAADvYGm5GjBih119/XUuXLlXlypWVlpYmSYqJiVFERIQkaeDAgapVq5ZSUlIkSRMmTFC7du3UoEEDZWRk6KmnntKePXs0dOhQ2z5HQVgKDgCAPWwNN9OnT5ckde7c2W/7nDlzNHjwYEnS3r175XTmnz07fPiwhg0bprS0NFWpUkWtW7fWZ599pqZNmwar28XCUnAAAOxh+2mps1mzZo3f86lTp2rq1KkB6pF1fAXFZBsAAILqvCgoLo8oKAYAwB6EmwDxFhSTbQAACC7CTYC4mLkBAMAWhJsAcZ5Sc1Oc2iIAAGANwk2AeAuKJVZMAQAQTISbAHGeGm6YuQEAIGgINwFy6swNZTcAAAQP4SZAvEvBJYqKAQAIJsJNgLiYuQEAwBaEmwBxMXMDAIAtCDcB4nQ65M03FBQDABA8hJsA8t1fiokbAACChnATQNxfCgCA4CPcBBAzNwAABB/hJoC8F/Jj5gYAgOAh3ASQy3d/KQqKAQAIFsJNAHlPS+UycQMAQNAQbgLIW1D8fXq2jh4/aXNvAACoGELs7kC5ZYxiQ07oiI5pzOufSZISYiKUeEElRUeEKiLMpYgQlyLCQhTqcshIMsbIewbL5XQo1OVUiMupUJdDIU6HXE6HnI78P4eFOBUW4lRoiFNul1MOOZRrjE56PMr1GHk8Rg6HQ06n8v57yoUFzR8HOvW4HpO/XZIckrwX63Hk/1EOh/zae9/jdDp9fQtxOvT7iVwdPZ6r33JO6ujxXBkjVQ53qXJ4qCqHhyrKHeJ3JWff/uXwO5ZDjlP+7O1z3uk+YyQjI6cj77gup0MuR9545nry+pZrjDweyenMC5x5j7z9eDxGJz3mj7Z/HOOPYzqd+X3J66bD99ml/J9Vft8dvj56x8bzR/9O5zilrfe93vYeY3yfzfszd/3xM8xrn/8z8bbN9eTVdnk/g3ccvHVf3j57b1B/6nGdjrz+5nryvzveY3t/lt5+nPpz0h+f0/PHd8Bj5Ouf05E31g5HwW2czvyfg/74WXk/h8cY32untzt9DItiTvkZFXVi2LsXR9G78zvm6W3NKT/nvD//8bsh/9/rvO9X3mfK/y4b33fa10an/rz9j+39XJ7T3ufXT4fyx9DhnUH2/i7kfedPb+M9nu8z6ZTvcDGOlfcd8b/1zOlj5O1z3ufJf+/p4+gdw1O/N/nfi/zf84J+Dt4xOpviVAsU9P+k0/ta0L4K75f3uTmtfd7YO0/5TIX17/TXT/+sBfW1OM72GQpqf+rnj49xyx3i8m8UWql4v1gBQLgJlBNH9f7vt0nhp2zLkfSzXR0CACCIHtonhUXacmhOSwEAgHKFmZtACa2Ul1oBAKiIQivZdmjCTaA4HLZNxwEAUJFxWgoAAJQrhBsAAFCuEG4AAEC5QrgBAADlCuEGAACUK4QbAABQrhBuAABAuUK4AQAA5QrhBgAAlCuEGwAAUK4QbgAAQLlCuAEAAOUK4QYAAJQrFe6u4MYYSVJWVpbNPQEAAMXl/Xvb+/d4USpcuDly5IgkqU6dOjb3BAAAnKsjR44oJiamyDYOU5wIVI54PB7t27dPlStXlsPhsHTfWVlZqlOnjlJTUxUdHW3pvpGHMQ48xjg4GOfAY4wDL5hjbIzRkSNHlJCQIKez6KqaCjdz43Q6Vbt27YAeIzo6ml+kAGOMA48xDg7GOfAY48AL1hifbcbGi4JiAABQrhBuAABAuUK4sZDb7dZjjz0mt9ttd1fKLcY48Bjj4GCcA48xDrzzdYwrXEExAAAo35i5AQAA5QrhBgAAlCuEGwAAUK4QbgAAQLlCuLHIc889p/r16ys8PFyXX3651q9fb3eXyqyUlBS1bdtWlStXVo0aNdS7d2/t3LnTr82xY8c0YsQIVatWTVFRUbrpppt04MABm3pc9k2ePFkOh0OjR4/2bWOMrfHzzz/r9ttvV7Vq1RQREaHmzZtr48aNvteNMXr00UdVs2ZNRUREqGvXrvruu+9s7HHZkpubq0ceeUSJiYmKiIjQRRddpIkTJ/rdf4gxPncff/yxbrjhBiUkJMjhcGjJkiV+rxdnTA8dOqQBAwYoOjpasbGxuuOOO5SdnR2cD2BQavPnzzdhYWHmpZdeMjt27DDDhg0zsbGx5sCBA3Z3rUzq1q2bmTNnjtm+fbvZsmWL6dGjh6lbt67Jzs72tbn77rtNnTp1zKpVq8zGjRtNu3btTPv27W3sddm1fv16U79+fZOUlGRGjRrl284Yl96hQ4dMvXr1zODBg83nn39udu3aZVasWGG+//57X5vJkyebmJgYs2TJErN161Zz4403msTERPP777/b2POyY9KkSaZatWrm7bffNrt37zYLFy40UVFR5plnnvG1YYzP3fLly83DDz9s3nzzTSPJLF682O/14oxp9+7dTYsWLcy6devM//73P9OgQQPTv3//oPSfcGOByy67zIwYMcL3PDc31yQkJJiUlBQbe1V+pKenG0nmo48+MsYYk5GRYUJDQ83ChQt9bb7++msjyaxdu9aubpZJR44cMQ0bNjQrV640nTp18oUbxtgaf/vb30zHjh0Lfd3j8Zj4+Hjz1FNP+bZlZGQYt9tt/vOf/wSji2Vez549zV/+8he/bX379jUDBgwwxjDGVjg93BRnTL/66isjyWzYsMHX5t133zUOh8P8/PPPAe8zp6VK6fjx49q0aZO6du3q2+Z0OtW1a1etXbvWxp6VH5mZmZKkqlWrSpI2bdqkEydO+I1548aNVbduXcb8HI0YMUI9e/b0G0uJMbbKsmXL1KZNG91yyy2qUaOGWrZsqRdeeMH3+u7du5WWluY3zjExMbr88ssZ52Jq3769Vq1apW+//VaStHXrVn3yySe67rrrJDHGgVCcMV27dq1iY2PVpk0bX5uuXbvK6XTq888/D3gfK9yNM632yy+/KDc3V3FxcX7b4+Li9M0339jUq/LD4/Fo9OjR6tChg5o1ayZJSktLU1hYmGJjY/3axsXFKS0tzYZelk3z58/XF198oQ0bNpzxGmNsjV27dmn69Om6//779dBDD2nDhg267777FBYWpkGDBvnGsqD/fzDOxfPggw8qKytLjRs3lsvlUm5uriZNmqQBAwZIEmMcAMUZ07S0NNWoUcPv9ZCQEFWtWjUo4064wXltxIgR2r59uz755BO7u1KupKamatSoUVq5cqXCw8Pt7k655fF41KZNGz3xxBOSpJYtW2r79u2aMWOGBg0aZHPvyocFCxZo3rx5ev3113XJJZdoy5YtGj16tBISEhjjCozTUqV0wQUXyOVynbGK5MCBA4qPj7epV+XDyJEj9fbbb2v16tWqXbu2b3t8fLyOHz+ujIwMv/aMefFt2rRJ6enpatWqlUJCQhQSEqKPPvpIzz77rEJCQhQXF8cYW6BmzZpq2rSp37YmTZpo7969kuQbS/7/UXJ//etf9eCDD+q2225T8+bN9ec//1n/7//9P6WkpEhijAOhOGMaHx+v9PR0v9dPnjypQ4cOBWXcCTelFBYWptatW2vVqlW+bR6PR6tWrVJycrKNPSu7jDEaOXKkFi9erA8//FCJiYl+r7du3VqhoaF+Y75z507t3buXMS+mLl26aNu2bdqyZYvv0aZNGw0YMMD3Z8a49Dp06HDGZQy+/fZb1atXT5KUmJio+Ph4v3HOysrS559/zjgX09GjR+V0+v9V5nK55PF4JDHGgVCcMU1OTlZGRoY2bdrka/Phhx/K4/Ho8ssvD3wnA16yXAHMnz/fuN1uM3fuXPPVV1+ZO++808TGxpq0tDS7u1YmDR8+3MTExJg1a9aY/fv3+x5Hjx71tbn77rtN3bp1zYcffmg2btxokpOTTXJyso29LvtOXS1lDGNshfXr15uQkBAzadIk891335l58+aZSpUqmddee83XZvLkySY2NtYsXbrUfPnll6ZXr14sUz4HgwYNMrVq1fItBX/zzTfNBRdcYP7v//7P14YxPndHjhwxmzdvNps3bzaSzJQpU8zmzZvNnj17jDHFG9Pu3bubli1bms8//9x88sknpmHDhiwFL2v+/e9/m7p165qwsDBz2WWXmXXr1tndpTJLUoGPOXPm+Nr8/vvv5p577jFVqlQxlSpVMn369DH79++3r9PlwOnhhjG2xltvvWWaNWtm3G63ady4sZk1a5bf6x6PxzzyyCMmLi7OuN1u06VLF7Nz506belv2ZGVlmVGjRpm6deua8PBwc+GFF5qHH37Y5OTk+Nowxudu9erVBf5/eNCgQcaY4o3pr7/+avr372+ioqJMdHS0GTJkiDly5EhQ+u8w5pTLOAIAAJRx1NwAAIByhXADAADKFcINAAAoVwg3AACgXCHcAACAcoVwAwAAyhXCDQAAKFcINwAqPIfDoSVLltjdDQAWIdwAsNXgwYPlcDjOeHTv3t3urgEoo0Ls7gAAdO/eXXPmzPHb5na7beoNgLKOmRsAtnO73YqPj/d7VKlSRVLeKaPp06fruuuuU0REhC688EItWrTI7/3btm3T1VdfrYiICFWrVk133nmnsrOz/dq89NJLuuSSS+R2u1WzZk2NHDnS7/VffvlFffr0UaVKldSwYUMtW7YssB8aQMAQbgCc9x555BHddNNN2rp1qwYMGKDbbrtNX3/9tSTpt99+U7du3VSlShVt2LBBCxcu1AcffOAXXqZPn64RI0bozjvv1LZt27Rs2TI1aNDA7xjjx4/Xrbfeqi+//FI9evTQgAEDdOjQoaB+TgAWCcrtOQGgEIMGDTIul8tERkb6PSZNmmSMybtL/N133+33nssvv9wMHz7cGGPMrFmzTJUqVUx2drbv9Xfeecc4nU6TlpZmjDEmISHBPPzww4X2QZL5+9//7nuenZ1tJJl3333Xss8JIHiouQFgu6uuukrTp0/321a1alXfn5OTk/1eS05O1pYtWyRJX3/9tVq0aKHIyEjf6x06dJDH49HOnTvlcDi0b98+denSpcg+JCUl+f4cGRmp6Ohopaenl/QjAbAR4QaA7SIjI884TWSViIiIYrULDQ31e+5wOOTxeALRJQABRs0NgPPeunXrznjepEkTSVKTJk20detW/fbbb77XP/30UzmdTjVq1EiVK1dW/fr1tWrVqqD2GYB9mLkBYLucnBylpaX5bQsJCdEFF1wgSVq4cKHatGmjjh07at68eVq/fr1efPFFSdKAAQP02GOPadCgQRo3bpwOHjyoe++9V3/+858VFxcnSRo3bpzuvvtu1ahRQ9ddd52OHDmiTz/9VPfee29wPyiAoCDcALDde++9p5o1a/pta9Sokb755htJeSuZ5s+fr3vuuUc1a9bUf/7zHzVt2lSSVKlSJa1YsUKjRo1S27ZtValSJd10002aMmWKb1+DBg3SsWPHNHXqVI0ZM0YXXHCBbr755uB9QABB5TDGGLs7AQCFcTgcWrx4sXr37m13VwCUEdTcAACAcoVwAwAAyhVqbgCc1zhzDuBcMXMDAADKFcINAAAoVwg3AACgXCHcAACAcoVwAwAAyhXCDQAAKFcINwAAoFwh3AAAgHKFcAMAAMqV/w8YXLma7kf5IQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PRelu(X, a):\n",
    "    return torch.max(torch.zeros_like(X), X) + torch.mul(a, torch.min(torch.zeros_like(X), X))\n",
    "\n",
    "\n",
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        mask = torch.bernoulli(torch.full_like(X, 1-p_drop))\n",
    "        return mask * X / (1.0 - p_drop)\n",
    "    return X\n",
    "\n",
    "def convolutional_model(X, w_c1, w_c2, w_c3 , w_h2 , w_o, p_drop):\n",
    "    # Convolutional layer 1\n",
    "    convolutional_layer_1 = rectify(conv2d(X, w_c1))\n",
    "    subsampling_layer_1 = max_pool2d(convolutional_layer_1, (2, 2))\n",
    "    out_layer_1 = dropout(subsampling_layer_1, p_drop)\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    convolutional_layer_2 = rectify(conv2d(out_layer_1, w_c2))\n",
    "    subsampling_layer_2 = max_pool2d(convolutional_layer_2, (2, 2))\n",
    "    out_layer_2 = dropout(subsampling_layer_2, p_drop)\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    convolutional_layer_3 = rectify(conv2d(out_layer_2, w_c3))\n",
    "    subsampling_layer_3 = max_pool2d(convolutional_layer_3, (2, 2))\n",
    "    out_layer_3 = dropout(subsampling_layer_3, p_drop)\n",
    "\n",
    "    # Flatten the matrix into a 1D tensor\n",
    "    flattened_input = torch.reshape(out_layer_3, (100,128))\n",
    "\n",
    "    h2 = rectify(flattened_input @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "# initialize weights\n",
    "number_of_output_pixels = 128\n",
    "\n",
    "# convolutional layers\n",
    "w_c1 = init_weights((32, 1, 5, 5))\n",
    "w_c2 = init_weights((64, 32, 5, 5))\n",
    "w_c3 = init_weights((128, 64, 3, 3))\n",
    "\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_c1, w_c2, w_c3, w_h2, w_o])\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        # feed input through model\n",
    "        noise_py_x = convolutional_model(x, w_c1, w_c2, w_c3, w_h2, w_o, p_drop=0.5)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(-1, 1, 28, 28)\n",
    "                noise_py_x = convolutional_model(x, w_c1, w_c2, w_c3, w_h2, w_o, p_drop=-1)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "calculate_test_error((-1, 1, 28, 28), convolutional_model, (x, w_c1, w_c2, w_c3, w_h2, w_o, -1))\n",
    "\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
