{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03: Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.distributions.binomial as binomial\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def init_prelu(shape, value=0.25):\n",
    "    return torch.full(shape, value, requires_grad=True)\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m784\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# feed input through model\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m noise_py_x \u001b[38;5;241m=\u001b[39m \u001b[43mdropout_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_h2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# reset the gradient\u001b[39;00m\n\u001b[0;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mdropout_model\u001b[1;34m(x, w_h, w_h2, w_o, a, a2, p_drop_input, p_drop_hidden)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdropout_model\u001b[39m(x, w_h, w_h2, w_o, a, a2, p_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, p_drop_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m     13\u001b[0m     h \u001b[38;5;241m=\u001b[39m PRelu(dropout(x \u001b[38;5;241m@\u001b[39m w_h, p_drop_input), a)\n\u001b[1;32m---> 14\u001b[0m     h2 \u001b[38;5;241m=\u001b[39m PRelu(\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_h2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_drop_hidden\u001b[49m\u001b[43m)\u001b[49m, a2)\n\u001b[0;32m     15\u001b[0m     pre_softmax \u001b[38;5;241m=\u001b[39m dropout(h2 \u001b[38;5;241m@\u001b[39m w_o)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pre_softmax\n",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(X, p_drop)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdropout\u001b[39m(X, p_drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m p_drop \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m         mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mp_drop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mask \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m p_drop)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def PRelu(X, a):\n",
    "    return torch.max(torch.zeros_like(X), X) + torch.mul(a, torch.min(torch.zeros_like(X), X))\n",
    "\n",
    "\n",
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        mask = torch.bernoulli(torch.full_like(X, 1-p_drop))\n",
    "        return mask * X / (1.0 - p_drop)\n",
    "    return X\n",
    "\n",
    "\n",
    "def dropout_model(x, w_h, w_h2, w_o, a, a2, p_drop_input=0.5, p_drop_hidden=0.5):\n",
    "    h = PRelu(dropout(x @ w_h, p_drop_input), a)\n",
    "    h2 = PRelu(dropout(h @ w_h2, p_drop_hidden), a2)\n",
    "    pre_softmax = dropout(h2 @ w_o)\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o, a, a2):\n",
    "    h = PRelu(x @ w_h, a)\n",
    "    h2 = PRelu(h @ w_h2, a2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "# Initialize PReLU parameter with Gaussian distribution\n",
    "a = init_prelu((1, w_h.shape[1]))\n",
    "a2 = init_prelu((1, w_h2.shape[1]))\n",
    "\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o, a, a2])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = dropout_model(x, w_h, w_h2, w_o, a, a2)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o, a, a2)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Dropout method works, and how it reduces overfitting\n",
    "Dropout is a technique where randomly selected neurons are ignored during training, meaning that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. The effect is that the network becomes less sensitive to the specific weights of neurons, and therefore less likely to overfit to the training data.\n",
    "\n",
    "### Difference with and without Dropout method\n",
    "With the implemented dropout_model() function, we observe significantly less overfitting. During the first run, the test data shows a loss value of about 0.8, while the training data performs much worse in comparison. Additionally, both training and test sets exhibit decreasing performance over the epochs, suggesting good initialization but poor training behavior.\n",
    "\n",
    "### Why do we need a different model configuration for evaluating the test loss?\n",
    "When evaluating the test loss, we use the original model without dropout. This is because dropout is a regularization technique used to prevent overfitting during training, and is not typically used during testing or inference. During testing, we want to use the full capacity of the model, so we don't apply dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Relu and PRelu\n",
    "We initialized a as proposed by Kaiming He et al. with 0.25. With the use of the Parametric Rectified Linear Unit we are observing essential improvment in the loss rate of our test data as we are approaching a loss of 0.2 as opposed of 0.8 using Relu. The model has also achieved better results for the train data with a loss of 1.2 instead of a loss of 2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 4.84e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 10\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 20\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 30\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 40\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 50\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 60\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 70\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 80\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 90\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n"
     ]
    }
   ],
   "source": [
    "def PRelu(X, a):\n",
    "    return torch.max(torch.zeros_like(X), X) + torch.mul(a, torch.min(torch.zeros_like(X), X))\n",
    "\n",
    "\n",
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        mask = torch.bernoulli(torch.full_like(X, 1-p_drop))\n",
    "        return mask * X / (1.0 - p_drop)\n",
    "    return X\n",
    "\n",
    "def convolutional_model(X, w_c1, w_c2, w_c3 , w_h2 , w_o, p_drop):\n",
    "    # Convolutional layer 1\n",
    "    convolutional_layer_1 = rectify(conv2d(X, w_c1))\n",
    "    subsampling_layer_1 = max_pool2d(convolutional_layer_1, (2, 2))\n",
    "    out_layer_1 = dropout(subsampling_layer_1, p_drop)\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    convolutional_layer_2 = rectify(conv2d(out_layer_1, w_c2))\n",
    "    subsampling_layer_2 = max_pool2d(convolutional_layer_2, (2, 2))\n",
    "    out_layer_2 = dropout(subsampling_layer_2, p_drop)\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    convolutional_layer_3 = rectify(conv2d(out_layer_2, w_c3))\n",
    "    subsampling_layer_3 = max_pool2d(convolutional_layer_3, (2, 2))\n",
    "    out_layer_3 = dropout(subsampling_layer_3, p_drop)\n",
    "\n",
    "    # Flatten the matrix into a 1D tensor\n",
    "    flattened_input = torch.reshape(out_layer_3, (100,128))\n",
    "\n",
    "    h2 = rectify(flattened_input @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "# initialize weights\n",
    "number_of_output_pixels = 128\n",
    "\n",
    "# convolutional layers\n",
    "w_c1 = init_weights((32, 1, 5, 5))\n",
    "w_c2 = init_weights((64, 32, 5, 5))\n",
    "w_c3 = init_weights((128, 64, 3, 3))\n",
    "\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_c1, w_c2, w_c3, w_h2, w_o])\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        # feed input through model\n",
    "        noise_py_x = convolutional_model(x, w_c1, w_c2, w_c3, w_h2, w_o, p_drop=0.5)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(-1, 1, 28, 28)\n",
    "                noise_py_x = convolutional_model(x, w_c1, w_c2, w_c3, w_h2, w_o, p_drop=-1)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
