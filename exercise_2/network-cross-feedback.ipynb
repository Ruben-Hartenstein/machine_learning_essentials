{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(0,input) # your code here\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        downstream_gradient = upstream_gradient * (self.input >0)  # your code here\n",
    "        # for inputs >0, derivatice of ReLU =1 , otherwise =0. Hence by doing so, downstream grad which corresponds to inputs < 0 would be turned \"off\" (\"masked\")\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "ReLu: Both solutions are basically the same.<br>\n",
    "Downstream_gradient: Both solutions are basically the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        exp_input = np.exp(input - np.max(input, axis=1, keepdims=True)) # for numerical stability\n",
    "        softmax = exp_input / np.sum(exp_input, axis=1, keepdims=True) # softmax = (exp{xi}/sum_{i} exp(xi))\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        num_examples = predicted_posteriors.shape[0]\n",
    "        downstream_gradient = predicted_posteriors.copy()\n",
    "        # downstream gradient = posterior_predictions - OneHot(true_predictions)\n",
    "        downstream_gradient[np.arange(num_examples), true_labels] -=1 \n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "Both solutions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        # using He initialization\n",
    "        variance_w = 2.0 / n_inputs  # Variance for weights\n",
    "        variance_b = 2.0 / n_inputs  # Variance for biases\n",
    "        self.B = np.random.normal(0, np.sqrt(variance_w), size=(n_inputs, n_outputs))\n",
    "        self.b = np.random.normal(0, np.sqrt(variance_b), size=(1, n_outputs))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations = np.dot(input, self.B) + self.b\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0, keepdims=True) # your code here\n",
    "        # grad_b's shape = (1, n_outputs)\n",
    "        self.grad_B =  np.dot(self.input.T, upstream_gradient) # your code here\n",
    "        # grad_B = z^T . ustream_grad, grad_B's shape = (n_inputs, n_outputs)\n",
    "        \n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)# your code here\n",
    "        # downstream_gradient = upstream_grad . B^T\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "Correclty used He Initialization which is also valid. <br>\n",
    "The rest is essentially the same as the provided solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        # your code here\n",
    "        \n",
    "        # We first initialize downstream_gradient with the gradient from the OutputLayer\n",
    "        downstream_gradient = self.layers[-1].backward(predicted_posteriors, true_classes)\n",
    "        # We then backpropagate through the remaining layers in reverse order\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            downstream_gradient = layer.backward(downstream_gradient)\n",
    "\n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate, print_after_num_epoch=5):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)\n",
    "\n",
    "            # calculate training error after each epoch # added by Neel\n",
    "            # Print training error after every 10 epochs\n",
    "            if (i + 1) %  print_after_num_epoch == 0:\n",
    "                predicted_posteriors = self.forward(x)\n",
    "                predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "                error_rate = np.mean(predicted_classes != y)\n",
    "                print(f\"Epoch {i+1}, Training Error Rate: {error_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "Both solutions are essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---layer size: [2, 2, 2]\n",
      "Epoch 10, Training Error Rate: 0.1215\n",
      "Epoch 20, Training Error Rate: 0.1115\n",
      "Epoch 30, Training Error Rate: 0.109\n",
      "Epoch 40, Training Error Rate: 0.1125\n",
      "Epoch 50, Training Error Rate: 0.1125\n",
      "Epoch 60, Training Error Rate: 0.1085\n",
      "Epoch 70, Training Error Rate: 0.112\n",
      "Epoch 80, Training Error Rate: 0.116\n",
      "Epoch 90, Training Error Rate: 0.113\n",
      "Epoch 100, Training Error Rate: 0.117\n",
      "error rate on val set: 0.1095\n",
      "---layer size: [3, 3, 2]\n",
      "Epoch 10, Training Error Rate: 0.111\n",
      "Epoch 20, Training Error Rate: 0.1105\n",
      "Epoch 30, Training Error Rate: 0.1125\n",
      "Epoch 40, Training Error Rate: 0.116\n",
      "Epoch 50, Training Error Rate: 0.1125\n",
      "Epoch 60, Training Error Rate: 0.118\n",
      "Epoch 70, Training Error Rate: 0.1125\n",
      "Epoch 80, Training Error Rate: 0.109\n",
      "Epoch 90, Training Error Rate: 0.113\n",
      "Epoch 100, Training Error Rate: 0.113\n",
      "error rate on val set: 0.107\n",
      "---layer size: [5, 5, 2]\n",
      "Epoch 10, Training Error Rate: 0.2055\n",
      "Epoch 20, Training Error Rate: 0.107\n",
      "Epoch 30, Training Error Rate: 0.1065\n",
      "Epoch 40, Training Error Rate: 0.102\n",
      "Epoch 50, Training Error Rate: 0.101\n",
      "Epoch 60, Training Error Rate: 0.1045\n",
      "Epoch 70, Training Error Rate: 0.108\n",
      "Epoch 80, Training Error Rate: 0.102\n",
      "Epoch 90, Training Error Rate: 0.1135\n",
      "Epoch 100, Training Error Rate: 0.102\n",
      "error rate on val set: 0.0975\n",
      "---layer size: [30, 30, 2]\n",
      "Epoch 10, Training Error Rate: 0.0385\n",
      "Epoch 20, Training Error Rate: 0.001\n",
      "Epoch 30, Training Error Rate: 0.0\n",
      "Epoch 40, Training Error Rate: 0.0\n",
      "Epoch 50, Training Error Rate: 0.0\n",
      "Epoch 60, Training Error Rate: 0.0\n",
      "Epoch 70, Training Error Rate: 0.0\n",
      "Epoch 80, Training Error Rate: 0.0\n",
      "Epoch 90, Training Error Rate: 0.0\n",
      "Epoch 100, Training Error Rate: 0.0\n",
      "error rate on val set: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes_list=[[2,2,n_classes], [3,3,n_classes], [5,5,n_classes], [30,30,n_classes]]\n",
    "    for i in range(0, len(layer_sizes_list)):\n",
    "        print(f\"---layer size: {layer_sizes_list[i]}\")\n",
    "        layer_sizes = layer_sizes_list[i]\n",
    "        n_epochs = 100\n",
    "        batch_size = 200\n",
    "        learning_rate = 0.001#0.05\n",
    "\n",
    "        # create network\n",
    "        network = MLP(n_features, layer_sizes)\n",
    "\n",
    "        # train\n",
    "        network.train(X_train, Y_train, n_epochs, batch_size, learning_rate, print_after_num_epoch=10)\n",
    "\n",
    "        # test\n",
    "        predicted_posteriors = network.forward(X_test)\n",
    "        # determine class predictions from posteriors by winner-takes-all rule\n",
    "        predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "        # compute and output the error rate of predicted_classes\n",
    "        error_rate = np.mean(predicted_classes != Y_test)\n",
    "        print(\"error rate on val set:\", error_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Comment</div>\n",
    "The solutions are essentially the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that if we fix n_epochs = 100, batch_size = 200 and learning_rate = 0.001, error rate on test set decreases on increasing the number of neurons in the inner layer. We also noticed that with these set of hyperparameters, for NN architecture [2,2,n_feature], sometimes training gets stuck and error rate stays 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
